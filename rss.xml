<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>Aurora</title>
        <link>https://charmer.fun</link>
        <description>Sidere mens eadem mutato</description>
        <generator>Zola</generator>
        <language>zh</language>
        <atom:link href="https://charmer.fun/rss.xml" rel="self" type="application/rss+xml"/>
        <lastBuildDate>Mon, 07 Mar 2022 14:37:06 +0800</lastBuildDate>
        <item>
            <title>通过例子快速入门C++</title>
            <pubDate>Mon, 07 Mar 2022 14:37:06 +0800</pubDate>
            <link>https://charmer.fun/posts/a-quick-introduction-to-c-plus-plus/</link>
            <guid>https://charmer.fun/posts/a-quick-introduction-to-c-plus-plus/</guid>
            <description>&lt;p&gt;⚠️ 本文为阅读笔记，&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;www.tfd.chalmers.se&#x2F;%7Ehani&#x2F;kurser&#x2F;OS_CFD_2007&#x2F;c++.pdf&quot;&gt;原文&lt;&#x2F;a&gt;为Tom Anderson对C++的&lt;strong&gt;子集&lt;&#x2F;strong&gt;的介绍。&lt;&#x2F;p&gt;
&lt;p&gt;在学习前需要解释的一点是，C++的内容可以划分为：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;基础部分，包含类、成员函数、构造器等等&lt;&#x2F;li&gt;
&lt;li&gt;进阶部分，包含单例继承、模版等等&lt;&#x2F;li&gt;
&lt;li&gt;垃圾部分，包含多重继承、异常、超载、引用等等&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;1是本文重点，每个C++程序员都应该掌握；2则是看到再查，但避免在自己的程序中使用；3则永远不要碰。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;c-bao-han-c&quot;&gt;C++包含C&lt;&#x2F;h2&gt;
&lt;p&gt;很大程度上，C++是C的超集，大部分符合ANSI的C程序都可以被C++编译器编译。不过有以下几个地方需要注意：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;所有函数都要先声明后使用。（在C中使用未声明的函数默认返回值是int）&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;所有函数的声明和定义头必须使用new-style声明，如&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;extern int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;foo&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;char&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;（在C中可以&lt;code&gt;extern int foo();&lt;&#x2F;code&gt;表示函数foo未知的参数数量和类型）&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;需要链接C的对象文件时，需要这样声明C函数：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;extern &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;quot;C&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;foo&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;char&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;C++包含需要新关键词，不能被用作标识符（identifiers）。它们是：&lt;code&gt;new&lt;&#x2F;code&gt;，&lt;code&gt;delete&lt;&#x2F;code&gt;，&lt;code&gt;const&lt;&#x2F;code&gt;和&lt;code&gt;class&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;ji-chu-bu-fen&quot;&gt;基础部分&lt;&#x2F;h2&gt;
&lt;p&gt;在开始讲解之前，下面三组面对对象语言的通用概念必须了解：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;class&lt;&#x2F;strong&gt; and &lt;strong&gt;object&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;member functions&lt;&#x2F;strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;constructor&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;destructor&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;private&lt;&#x2F;strong&gt; vs. &lt;strong&gt;public&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;接下来用例子描述C++中的各类基础特性。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;lei&quot;&gt;类&lt;&#x2F;h3&gt;
&lt;p&gt;为了直观展示C++中的类使用，将以栈类的代码实现为例子。&lt;&#x2F;p&gt;
&lt;p&gt;在C++中，类的定义和实现是分离的（有种特殊情况，后文提到）。通常，在&lt;code&gt;.h&lt;&#x2F;code&gt;头文件中存放类的定义，在&lt;code&gt;.cc&lt;&#x2F;code&gt;文件中存放类的成员函数实现。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; stack.h
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Stack &lt;&#x2F;span&gt;&lt;span&gt;{
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt; stack;            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.1
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;public&lt;&#x2F;span&gt;&lt;span&gt;:                    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.2
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Stack&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;sz&lt;&#x2F;span&gt;&lt;span&gt;);         &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.3
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;~Stack&lt;&#x2F;span&gt;&lt;span&gt;();              &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.4
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;void &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Push&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;);  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.5
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;bool &lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Full&lt;&#x2F;span&gt;&lt;span&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;const &lt;&#x2F;span&gt;&lt;span&gt;{ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;(top &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;==&lt;&#x2F;span&gt;&lt;span&gt; size); };     &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 1.6
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;private&lt;&#x2F;span&gt;&lt;span&gt;:                   &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 7
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;private&lt;&#x2F;span&gt;&lt;span&gt;:                   &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 7
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt; top;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt; size;
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;cheng-yuan-han-shu&quot;&gt;成员函数&lt;&#x2F;h4&gt;
&lt;p&gt;在stack.h头文件中&lt;em&gt;1.1&lt;&#x2F;em&gt;就是&lt;code&gt;Stack&lt;&#x2F;code&gt;类的普通成员函数。我们给出它的实现：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; stack.cc
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;void
&lt;&#x2F;span&gt;&lt;span&gt;Stack::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Push&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;value&lt;&#x2F;span&gt;&lt;span&gt;) {         &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 2.1
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;ASSERT(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;!&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;this-&amp;gt;Full())&lt;&#x2F;span&gt;&lt;span&gt;;       &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 2.2
&lt;&#x2F;span&gt;&lt;span&gt;    stack[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;this&lt;&#x2F;span&gt;&lt;span&gt;-&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;top&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;++&lt;&#x2F;span&gt;&lt;span&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; value;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 2.3
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;首先看到&lt;em&gt;2.1&lt;&#x2F;em&gt;，&lt;em&gt;class::function&lt;&#x2F;em&gt;的标识符表明&lt;code&gt;Push&lt;&#x2F;code&gt;是类的成员函数。&lt;&#x2F;p&gt;
&lt;p&gt;在&lt;em&gt;2.2&lt;&#x2F;em&gt;中，成员函数体内可以直接使用对象的成员，这里访问了&lt;code&gt;Full()&lt;&#x2F;code&gt;函数。C++可以通过&lt;code&gt;-&amp;gt;&lt;&#x2F;code&gt;操作符访问对象的成员，同时，在成员函数内可以通过&lt;code&gt;this&lt;&#x2F;code&gt;变量指代当前对象，通常&lt;code&gt;this&lt;&#x2F;code&gt;是隐式的，因此可以改写成&lt;code&gt;ASSERT(!Full());&lt;&#x2F;code&gt;，&lt;em&gt;2.3&lt;&#x2F;em&gt;即是隐式调用对象成员。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;si-you-cheng-yuan&quot;&gt;私有成员&lt;&#x2F;h4&gt;
&lt;p&gt;面对对象编程自然必须涉及封装。C++提供了私有成员的方式实现封装。默认情况下C++类成员为私有成员，如&lt;em&gt;1.1&lt;&#x2F;em&gt;就是私有的，成员函数内可以随意使用，但外部不可见。&lt;&#x2F;p&gt;
&lt;p&gt;可以显示使用&lt;code&gt;public&lt;&#x2F;code&gt;关键词表明后续成员为公有成员，如&lt;em&gt;1.2&lt;&#x2F;em&gt;所示。&lt;&#x2F;p&gt;
&lt;p&gt;对之对应，可以使用&lt;code&gt;private&lt;&#x2F;code&gt;关键词表明后续成员为私有成员。&lt;&#x2F;p&gt;
&lt;p&gt;❤️ 所有包含数据的成员变量都应当私有。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;gou-zao-han-shu-he-newcao-zuo-fu&quot;&gt;构造函数和new操作符&lt;&#x2F;h4&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; stack.cc
&lt;&#x2F;span&gt;&lt;span&gt;Stack::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Stack&lt;&#x2F;span&gt;&lt;span&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;sz&lt;&#x2F;span&gt;&lt;span&gt;) {
&lt;&#x2F;span&gt;&lt;span&gt;    size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; sz;
&lt;&#x2F;span&gt;&lt;span&gt;    top &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;    stack &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;new int&lt;&#x2F;span&gt;&lt;span&gt;[size];  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 3.1
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;em&gt;1.3&lt;&#x2F;em&gt;即是Stack类的构造函数，构造函数用于给对象成员赋予默认值，并进行一些初始化操作。在C++中，使用&lt;code&gt;new&lt;&#x2F;code&gt;关键词进行对象创建，它会自动调用类的构造函数。这个流程同样适用于自动变量。&lt;&#x2F;p&gt;
&lt;p&gt;对于类来说构造函数是必须的，若没有显示定义，编译器会给你加上一个空构造函数。&lt;&#x2F;p&gt;
&lt;p&gt;有两种方式传递函数参数给构造函数：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;void
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;test&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 通过new关键词（堆上变量）
&lt;&#x2F;span&gt;&lt;span&gt;    Stack&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt; s1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;new &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;Stack(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 4.1
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 自动变量或全局变量（栈上变量）
&lt;&#x2F;span&gt;&lt;span&gt;    Stack &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;s2(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;               &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 4.2
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;函数内的变量在函数返回时会自动释放，比如&lt;em&gt;4.2&lt;&#x2F;em&gt;定义的s2就会在test()函数返回时释放。但是通过new关键词创建的变量（如&lt;em&gt;4.1&lt;&#x2F;em&gt;)存储在堆（heap）上，即是函数返回仍然存在，必须手动delete。&lt;&#x2F;p&gt;
&lt;p&gt;new关键词也可以用来分配数组，如&lt;em&gt;3.1&lt;&#x2F;em&gt;所示。同样需要手动delete。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xi-gou-han-shu-he-deletecao-zuo-fu&quot;&gt;析构函数和delete操作符&lt;&#x2F;h4&gt;
&lt;pre data-lang=&quot;c++&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-c++ &quot;&gt;&lt;code class=&quot;language-c++&quot; data-lang=&quot;c++&quot;&gt;&lt;span&gt;Stack::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;~Stack&lt;&#x2F;span&gt;&lt;span&gt;() {
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;delete []&lt;&#x2F;span&gt;&lt;span&gt; stack;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; 5.1
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;有构造函数就有析构函数，就如有new就有delete。&lt;&#x2F;p&gt;
&lt;p&gt;对于new出来的对象，使用delete手动释放，如&lt;em&gt;4.1&lt;&#x2F;em&gt;的s1：&lt;code&gt;delete d1;&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;对于new出来的数组，也使用delete手动释放，如&lt;em&gt;5.1&lt;&#x2F;em&gt;所示。注意中间的&lt;code&gt;[]&lt;&#x2F;code&gt;表明释放数组，而不是数组中单个变量。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;qi-ta-c-de-ji-chu-te-xing&quot;&gt;其他C++的基础特性&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;类可以作为数据类型使用。&lt;&#x2F;li&gt;
&lt;li&gt;事实上可以在类是声明时（头文件）内同时声明实现函数，如&lt;em&gt;1.6&lt;&#x2F;em&gt;所示。但通常要求函数体简单。这么做有两个好处：方便和性能。在类声明时实现的函数会被当作&lt;code&gt;inline&lt;&#x2F;code&gt;函数。&lt;&#x2F;li&gt;
&lt;li&gt;const关键词除了像C一样声明变量时使用，表示变量不可变，还可以用于函数声明，表明函数对数据只读而不会修改，如&lt;em&gt;1.6&lt;&#x2F;em&gt;。&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;cin&lt;&#x2F;code&gt;和&lt;code&gt;cout&lt;&#x2F;code&gt;对象是C++中的标准输入输出。使用&lt;code&gt;&amp;gt;&amp;gt;&lt;&#x2F;code&gt;和&lt;code&gt;&amp;lt;&amp;lt;&lt;&#x2F;code&gt;分别输入、输出。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;jin-jie-bu-fen&quot;&gt;进阶部分&lt;&#x2F;h2&gt;
&lt;p&gt;这部分内容容易误用，但如果用得恰当可以大幅简化逻辑。&lt;&#x2F;p&gt;
&lt;p&gt;后面内容由于博主没有使用场景，暂时不学习。😛&lt;&#x2F;p&gt;
&lt;h3 id=&quot;dan-li-ji-cheng&quot;&gt;（单例）继承&lt;&#x2F;h3&gt;
&lt;h3 id=&quot;mo-ban&quot;&gt;模版&lt;&#x2F;h3&gt;
&lt;h2 id=&quot;la-ji-bu-fen&quot;&gt;垃圾部分&lt;&#x2F;h2&gt;
</description>
        </item>
        <item>
            <title>rust的trait和trait对象</title>
            <pubDate>Mon, 07 Mar 2022 14:37:06 +0800</pubDate>
            <link>https://charmer.fun/posts/rust-traits-and-trait-objects/</link>
            <guid>https://charmer.fun/posts/rust-traits-and-trait-objects/</guid>
            <description></description>
        </item>
        <item>
            <title>HyperLogLog</title>
            <pubDate>Sun, 27 Feb 2022 17:15:52 +0800</pubDate>
            <link>https://charmer.fun/posts/hyper-log-log/</link>
            <guid>https://charmer.fun/posts/hyper-log-log/</guid>
            <description>&lt;p&gt;TODO&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>2 可扩展布隆过滤器</title>
            <pubDate>Tue, 15 Feb 2022 14:00:16 +0800</pubDate>
            <link>https://charmer.fun/posts/bloom-filter-2/</link>
            <guid>https://charmer.fun/posts/bloom-filter-2/</guid>
            <description>&lt;h2 id=&quot;jie-shao&quot;&gt;介绍&lt;&#x2F;h2&gt;
&lt;p&gt;在上篇博文&lt;a href=&quot;&#x2F;posts&#x2F;bloom-filter-1&#x2F;&quot;&gt;基础布隆过滤器&lt;&#x2F;a&gt;中介绍了基础的bloom filter以及相关数学推导，最后给出了应用最佳实践。在文章最后提及基础布隆过滤器无法在保证false positive率的同时扩展存储，必须在使用前预估集合大小（n）。但是在很多情况下，集合大小无法预估，这就需要使用本篇介绍的布隆过滤器变体。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bu-long-guo-lu-qi-bian-ti&quot;&gt;布隆过滤器变体&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;oliverdding&#x2F;imaw.io&#x2F;main&#x2F;variant_bloom_filter.drawio.svg&quot; alt=&quot;布隆过滤器变体&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;布隆过滤器变体由以下两部分构成：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;M位的位图：&lt;code&gt;bitmap&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;k个相互独立的hash函数：&lt;code&gt;h1, h2, ..., hk&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;其中，M位的位图被k等分，每个slice长度为m，即$m=\frac{M}{k}$。&lt;&#x2F;p&gt;
&lt;p&gt;与基础布隆过滤器不同的是，每个hash函数独享自己对应的slice（而不是全部位图）。&lt;&#x2F;p&gt;
&lt;p&gt;这种变体提高了健壮性，并且支持并发处理。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-xue-tui-dao&quot;&gt;数学推导&lt;&#x2F;h3&gt;
&lt;p&gt;对于某个slice，插入1个元素后，某一个位被置1、置0的概率为$\frac{1}{m}$、$1-\frac{1}{m}$&lt;&#x2F;p&gt;
&lt;p&gt;当插入n个元素后，某一位被置0、置1的概率是$(1-\frac{1}{m})^n$、$1-(1-\frac{1}{m})^n$&lt;&#x2F;p&gt;
&lt;p&gt;同样使用自然对数e的计算公式$$\lim_{x \to \infty}{(1-\frac{1}{x})^{-x}}=e$$&lt;&#x2F;p&gt;
&lt;p&gt;插入n个元素后某一位被置1的概率可以简化为$$p\approx{}1-e^{-\frac{n}{m}}$$&lt;&#x2F;p&gt;
&lt;p&gt;于是得到$$n\approx{}-mln(1-p)$$&lt;&#x2F;p&gt;
&lt;p&gt;又由公式$M=km$和$P=p^k$，我们推导出$m=M\frac{ln(p)}{ln(P)}$，因此有$$n\approx{}M\frac{ln(p)ln(1-p)}{-ln(P)}$$&lt;&#x2F;p&gt;
&lt;p&gt;对于给定的纳伪率$P$和位图大小M，当$p=\frac{1}{2}$时n取到最大值，此时能容纳最多的元素。$p$代表一个slice的饱和程度，我们得知slice半饱和时最好，对于$p=\frac{1}{2}$我们可以简化上面的公式为$$n\approx{}M\frac{(ln(2)^2)}{\lvert{}ln(P)\rvert{}}$$&lt;&#x2F;p&gt;
&lt;p&gt;P是给定的纳伪率，因此对于这个布隆过滤器变体而言，可以容纳的元素数量n是和位图大小M呈线性关系。&lt;&#x2F;p&gt;
&lt;p&gt;最后由$P=p^k$和$p=\frac{1}{2}$我们推导出$$k=log_{2}\frac{1}{P}$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ying-yong&quot;&gt;应用&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;yi-zhi-n-he-p&quot;&gt;已知$n$和$P$&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;根据公式$k=\lceil{}log_{2}\frac{1}{P}\rceil{}$计算k的值&lt;&#x2F;li&gt;
&lt;li&gt;根据公式$m\approx{}\lceil{}\frac{n\lvert{}ln(P)\rvert{}}{kln^2(2)}\rceil{}$计算m的值&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;ke-kuo-zhan-de-bu-long-guo-lu-qi&quot;&gt;可扩展的布隆过滤器&lt;&#x2F;h2&gt;
&lt;p&gt;本篇介绍的可扩展布隆过滤器（Scalable Bloom Filter, SBF）基于前文提及的布隆过滤器变体，有两个关键部分：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;SBF由一个或多个布隆过滤器组成。&lt;&#x2F;li&gt;
&lt;li&gt;后续的布隆过滤器的纳伪率成等比数列缩小，以保证整体纳伪率保持在一个数值内。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;可扩展布隆过滤器同样存在两个操作：插入、查询。&lt;&#x2F;p&gt;
&lt;p&gt;当插入时，从过滤器列表最后一个过滤器插入新元素，并在“满了”时扩展过滤器列表。&lt;&#x2F;p&gt;
&lt;p&gt;但查询时，从前往后依次查询，每个过滤器都返回不存在时，则表明元素不存在，否则返回存在。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shu-xue-tui-dao-1&quot;&gt;数学推导&lt;&#x2F;h3&gt;
&lt;p&gt;SBF创建时只有一个布隆过滤器，$k_0$个slice，纳伪率为$P_0$。当这个过滤器满了，添加新的过滤器（$k_1$个slice，纳伪率为$P_1=P_{0}r$，且$0&amp;lt;r&amp;lt;1$）。&lt;&#x2F;p&gt;
&lt;p&gt;当SBF存在$l$个过滤器后，其纳伪率为$$P=1-\prod_{i=0}^{l-1}(1-P_{0}r^i)$$&lt;&#x2F;p&gt;
&lt;p&gt;由于有缩放公式$$1-\prod_{i}(1-P_i)\le{}\sum_{i}P_i$$&lt;&#x2F;p&gt;
&lt;p&gt;我们可以得到P的上限$$P\le{}\sum_{i=0}^{l-1}P_{0}r^{i}\le{}\lim_{l \to \infty}\sum_{i=0}^{l-1}P_{0}r^i$$&lt;&#x2F;p&gt;
&lt;p&gt;因此有$$P\le{}P_{0}\frac{1}{1-r}$$&lt;&#x2F;p&gt;
&lt;p&gt;每个过滤器的slice长度为$$k_{0}=log_{2}(P_{0}^{-1})$$，$$k_{i}=log_{2}(P_{i}^{-1})=k_{0}+ilog_{2}(r^{-1}) \tag{1}$$&lt;&#x2F;p&gt;
&lt;p&gt;为了保证每个k_i都是整数，我们自然会想让$r=\frac{1}{2}$，此时有$$k_i=k_0+i$$，意味着每添加一个过滤器都需要扩充一个slice。此时SBF的整体纳伪率为$$P\le{}2P_0=2^{1-k_0}$$&lt;&#x2F;p&gt;
&lt;p&gt;论文证明了如果$r\lt{}0.5$，尽管小量数据时使用空间大于$r=0.5$，随着空间增长空间使用反而更小。论文实验证明，r取0.8～0.9时为最佳。[1]&lt;&#x2F;p&gt;
&lt;p&gt;事实上，后续添加的过滤器的纳伪率$P_i$只要低于整体纳伪率，则可保证整体纳伪率不增加。因此$k_i$的选取可以更加灵活，而不局限于(1)的公式。由于$P_i$和$k_i$成反比，只要$k_i$选取大于(1)的值即可保证$P_i$小。这里为了简化我们可以使用等比数列。假设k的取值为$P_0, P_{0}r, P_{0}r^2,...,P_{0}r^{l-1}$，此时过滤器可以存储的变量数$n_i$大约为$$n_i\approx{}m_{0}s^{i}ln(2)$$，有$l$个过滤器的SBF可以存储约$(ln2)m_{0}\sum_{i=0}^{l-1}s^i$个元素。&lt;&#x2F;p&gt;
&lt;p&gt;论文证明对于缓慢增长的SBF可以考虑设置$s=2$，对于快速增长的过滤器可以设置$s=4$。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ying-yong-1&quot;&gt;应用&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;yi-zhi-p&quot;&gt;已知$P$&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;考虑使用$n=8192$构造初始布隆过滤器变体，计算获得$k_0$和m&lt;&#x2F;li&gt;
&lt;li&gt;对于每个后续的$P_i=0.9P_{i-1}$&lt;&#x2F;li&gt;
&lt;li&gt;对于每个后续的过滤器，$k_i=2k_{i-1}$或$k_i=4k_{i-1}$&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;can-kao&quot;&gt;参考&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]: Scalable Bloom Filters&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>使用静态网站写博客的最佳实践</title>
            <pubDate>Fri, 11 Feb 2022 20:37:06 +0800</pubDate>
            <link>https://charmer.fun/posts/best-practice-for-writing-blog-with-ssg/</link>
            <guid>https://charmer.fun/posts/best-practice-for-writing-blog-with-ssg/</guid>
            <description>&lt;p&gt;我尝试过hexo，用过hugo，最终转向zola这个完全还没发展起来的SSG。这篇文章记录下使用SSG写博客的最佳实践。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zhu-ti-xuan-ze&quot;&gt;主题选择&lt;&#x2F;h2&gt;
&lt;p&gt;主题选择驱使我从hugo迁移到zola。我对主题的要求非常苛刻：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;积极维护中，并且有庞大的用户群体。&lt;&#x2F;li&gt;
&lt;li&gt;样式简洁美观，我倾向于fluent design。&lt;&#x2F;li&gt;
&lt;li&gt;功能简单但不单调
&lt;ol&gt;
&lt;li&gt;支持latex渲染&lt;&#x2F;li&gt;
&lt;li&gt;支持toc&lt;&#x2F;li&gt;
&lt;li&gt;（可选）支持评论&lt;&#x2F;li&gt;
&lt;li&gt;（可选）支持admonition&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;我极其讨厌复杂庞大的可选项（没错，我也讨厌kde），所以配置文件越精简越好。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;在hugo中，我从Loveit（似乎停止维护了）搬迁到papermod（非常惊艳，但是公式渲染需要html代码配置，并且渲染异常），再尝试了stack（太花哨）、terminal（风格不喜）、MemE（配置项过于庞大），我逃离到了zola。再一番抉择后发现了现在正在使用的主题&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;RatanShreshtha&#x2F;DeepThought&quot;&gt;DeepThought&lt;&#x2F;a&gt;，它是zola中几乎最流行的主题，界面简洁优美，配置精简，原生支持渲染，toc可选，支持评论，唯一的缺点就是缺少中文搜索的支持。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bo-ke-yuan-wen-ben-cun-chu&quot;&gt;博客源文本存储&lt;&#x2F;h2&gt;
&lt;p&gt;首先，博客的源码肯定是要纳入版本管理的，理所当然需要给它们创建一个仓库。&lt;&#x2F;p&gt;
&lt;p&gt;这里有一些约定：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;只有网站本身使用的图片（图标、头像）放到static文件夹中，其他博文图片全部使用图床的方式+markdown链接引用。&lt;&#x2F;li&gt;
&lt;li&gt;画图尽量使用mermaid、plantuml、draw.io这些纯文图图片来记录。draw.io同样存储于图床。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;bo-ke-jing-tai-wang-zhan-cun-chu&quot;&gt;博客静态网站存储&lt;&#x2F;h2&gt;
&lt;p&gt;想要使用Github Pages，需要创建一个仓库用于存储build出来的静态网站文件。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bo-ke-bian-yi-zi-dong-hua&quot;&gt;博客编译自动化&lt;&#x2F;h2&gt;
&lt;p&gt;现在源码在仓库A中，编译的静态网站在仓库B中，每次写完博文都需要手动build下然后推送仓库B、仓库A，这当然无法忍受。&lt;&#x2F;p&gt;
&lt;p&gt;于是可以利用Github Action配置流水线，每次提交到仓库A都会触发流水线，自动编译发布仓库B。&lt;&#x2F;p&gt;
&lt;p&gt;这里涉及到仓库A的编译发布流水线提交git到仓库B的权限问题，github可以使用token来解决。&lt;&#x2F;p&gt;
&lt;p&gt;但是我更倾向于使用仓库级密钥：&lt;&#x2F;p&gt;
&lt;p&gt;通过以下命令创建一对SSh key：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;ssh-keygen -t rsa -b 4096 -C &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;quot;$(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;git config user.email&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;)&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; -f gh-pages -N &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;将生成的gh-pages.pub公钥配置仓库B，允许写入。并将生成的gh-pages私钥配置仓库A的私有变量，我将它命名为&lt;code&gt;ZOLA&lt;&#x2F;code&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;这是我zola的配置，&lt;code&gt;.github&#x2F;workflows&#x2F;deploy-website.yml&lt;&#x2F;code&gt;：&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-yaml &quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;CI
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;on&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;branches&lt;&#x2F;span&gt;&lt;span&gt;: [ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;main &lt;&#x2F;span&gt;&lt;span&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;workflow_dispatch&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;jobs&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;build-and-deploy&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;runs-on&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;ubuntu-latest
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;steps&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;Checkout the code
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;uses&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;actions&#x2F;checkout@master
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;with&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;submodules&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;true
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;Build Static Website
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;uses&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;shalzz&#x2F;zola-deploy-action@master
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;env&lt;&#x2F;span&gt;&lt;span&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;BUILD_ONLY&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;true
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;BUILD_DIR&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;.
&lt;&#x2F;span&gt;&lt;span&gt;      - &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;Deploy Static Website
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;uses&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;peaceiris&#x2F;actions-gh-pages@v3
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;with&lt;&#x2F;span&gt;&lt;span&gt;: 
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;deploy_key&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;${{ secrets.ZOLA }}
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;external_repository&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;oliverdding&#x2F;oliverdding.github.io
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;publish_branch&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;main
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;publish_dir&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;.&#x2F;public
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;user_name&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;#39;github-actions[bot]&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;user_email&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;#39;github-actions[bot]@users.noreply.github.com&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;font-weight:bold;color:#8ec07c;&quot;&gt;full_commit_message&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;${{ github.event.head_commit.message }}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;shalzz&#x2F;zola-deploy-action也可以直接推送，但是我想K.I.S.S些，就职责分离了。hexo、hugo大体相同，只要改动第二部build即可。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;kuo-zhan&quot;&gt;扩展&lt;&#x2F;h2&gt;
&lt;p&gt;当然，也可以选择第三方发布平台部署静态网站，比如比较出名的&lt;a href=&quot;netlify.com&quot;&gt;netify&lt;&#x2F;a&gt;和&lt;a href=&quot;vercel.com&quot;&gt;vercel&lt;&#x2F;a&gt;，此时只需要源码仓库即可编译发布静态博客。&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>1 基础布隆过滤器</title>
            <pubDate>Wed, 09 Feb 2022 17:13:10 +0800</pubDate>
            <link>https://charmer.fun/posts/bloom-filter-1/</link>
            <guid>https://charmer.fun/posts/bloom-filter-1/</guid>
            <description>&lt;h2 id=&quot;jie-shao&quot;&gt;介绍&lt;&#x2F;h2&gt;
&lt;p&gt;布隆过滤器（Bloom Filter）用于判断某个元素是否在集合中的一个简单、高效、内存占用低的数据结构，它类似于哈希表，相比于后者，它存在false positive值，但显著地降低了内存占用。这种&lt;a href=&quot;&#x2F;categories&#x2F;probabilisticdatastructure&#x2F;&quot;&gt;概率性数据结构&lt;&#x2F;a&gt;值得体会，以退为进，&lt;em&gt;只要false positive控制在可以允许的范围&lt;&#x2F;em&gt;，资源消耗被大幅降低[4]。&lt;&#x2F;p&gt;
&lt;p&gt;使用布隆过滤器查找key时，返回值可能情况有两种：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;key&lt;strong&gt;不&lt;&#x2F;strong&gt;存在&lt;&#x2F;li&gt;
&lt;li&gt;key&lt;strong&gt;可能&lt;&#x2F;strong&gt;存在（存在、纳伪）&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;使用布隆过滤器原则：false positive影响可控。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;li-shi&quot;&gt;历史&lt;&#x2F;h3&gt;
&lt;p&gt;布隆过滤器于1970年代被Burton Bloom创造，在查询集合成员存在与否时一般考虑时间成本或空间成本，论文提出第三个优化方向：Allowable Fraction of Errors，也就是说允许一定的误判，来大幅降低空间占用[1]。之后一段时间布隆过滤器被广泛运用在数据库领域。&lt;&#x2F;p&gt;
&lt;p&gt;之后又有人提出利用布隆过滤器+共享缓存的方式大幅降低缓存服务器的带宽[2]，自此布隆过滤器在互联网中展现实力[3]。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;yuan-li&quot;&gt;原理&lt;&#x2F;h3&gt;
&lt;p&gt;布隆过滤器由以下三部分构成：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;n个key&lt;&#x2F;li&gt;
&lt;li&gt;m位的位图：&lt;code&gt;bitmap&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;k个相互独立的hash函数：&lt;code&gt;h1, h2, ..., hk&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;可以用$h_{i}(x)=MD5(x+i)$或者$h_{i}(x)=MD5(x|i)$实现k个无关hash函数&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;需要插入key时，对&lt;code&gt;key = a&lt;&#x2F;code&gt;时，经过k个hash函数后得到k个数&lt;code&gt;h1(a), h2(a), ..., hk(a)&lt;&#x2F;code&gt;，此时将&lt;code&gt;bitmap&lt;&#x2F;code&gt;中对应的位置1。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;oliverdding&#x2F;imaw.io&#x2F;main&#x2F;inserting-key-into-bloom-filter.drawio.svg&quot; alt=&quot;往布隆过滤器插入新key&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;需要查询key时，同样经过k个hash函数得到k个体数，查询&lt;code&gt;bitmap&lt;&#x2F;code&gt;中对应位，若全为1，则key&lt;strong&gt;可能&lt;&#x2F;strong&gt;存在；若任意位为0，则&lt;strong&gt;一定不&lt;&#x2F;strong&gt;存在。（这里可以利用短路减少消耗，若出现0则直接返回不存在）&lt;&#x2F;p&gt;
&lt;p&gt;当key越来越多，&lt;code&gt;bitmap&lt;&#x2F;code&gt;中为1的位越多，对于某个不存在的key，k个hash函数得到的坐标对应&lt;code&gt;bitmap&lt;&#x2F;code&gt;中的值都为1的概率越大，此时就发生false positive。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;li-zi&quot;&gt;例子&lt;&#x2F;h3&gt;
&lt;p&gt;为了直观描述布隆过滤器如何加速查询的，我们用数据库场景来描述。&lt;&#x2F;p&gt;
&lt;p&gt;我们对表中某个字段建立bloom filter，假设有10位的bitmap如下所示：&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;0&lt;&#x2F;th&gt;&lt;th&gt;1&lt;&#x2F;th&gt;&lt;th&gt;2&lt;&#x2F;th&gt;&lt;th&gt;3&lt;&#x2F;th&gt;&lt;th&gt;4&lt;&#x2F;th&gt;&lt;th&gt;5&lt;&#x2F;th&gt;&lt;th&gt;6&lt;&#x2F;th&gt;&lt;th&gt;7&lt;&#x2F;th&gt;&lt;th&gt;8&lt;&#x2F;th&gt;&lt;th&gt;9&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;我们使用Fnv和Murmur这两个hash函数。&lt;&#x2F;p&gt;
&lt;p&gt;首先插入字符串&lt;code&gt;hello&lt;&#x2F;code&gt;，计算&lt;code&gt;Fnv=6&lt;&#x2F;code&gt;，&lt;code&gt;Murmur=0&lt;&#x2F;code&gt;，于是bitmap如下所示：&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;0&lt;&#x2F;th&gt;&lt;th&gt;1&lt;&#x2F;th&gt;&lt;th&gt;2&lt;&#x2F;th&gt;&lt;th&gt;3&lt;&#x2F;th&gt;&lt;th&gt;4&lt;&#x2F;th&gt;&lt;th&gt;5&lt;&#x2F;th&gt;&lt;th&gt;6&lt;&#x2F;th&gt;&lt;th&gt;7&lt;&#x2F;th&gt;&lt;th&gt;8&lt;&#x2F;th&gt;&lt;th&gt;9&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;接着插入字符串&lt;code&gt;world&lt;&#x2F;code&gt;，计算&lt;code&gt;Fnv=7&lt;&#x2F;code&gt;，&lt;code&gt;Murmur=1&lt;&#x2F;code&gt;，于是bitmap如下所示：&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;0&lt;&#x2F;th&gt;&lt;th&gt;1&lt;&#x2F;th&gt;&lt;th&gt;2&lt;&#x2F;th&gt;&lt;th&gt;3&lt;&#x2F;th&gt;&lt;th&gt;4&lt;&#x2F;th&gt;&lt;th&gt;5&lt;&#x2F;th&gt;&lt;th&gt;6&lt;&#x2F;th&gt;&lt;th&gt;7&lt;&#x2F;th&gt;&lt;th&gt;8&lt;&#x2F;th&gt;&lt;th&gt;9&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;td&gt;0&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;此时我们查询&lt;code&gt;charmer&lt;&#x2F;code&gt;是否存在，计算&lt;code&gt;Fnv=12&lt;&#x2F;code&gt;，&lt;code&gt;Murmur=10&lt;&#x2F;code&gt;，bitmap中存在0，因此&lt;strong&gt;不存在&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;再查询&lt;code&gt;hello&lt;&#x2F;code&gt;是否存在，计算&lt;code&gt;Fnv=6&lt;&#x2F;code&gt;，&lt;code&gt;Murmur=0&lt;&#x2F;code&gt;，bitmap中都为1，因此&lt;strong&gt;可能存在&lt;&#x2F;strong&gt;，需要访问磁盘，得知真的存在。&lt;&#x2F;p&gt;
&lt;p&gt;再查询&lt;code&gt;zaslf&lt;&#x2F;code&gt;是否存在，计算&lt;code&gt;Fnv=7&lt;&#x2F;code&gt;，&lt;code&gt;Murmur=1&lt;&#x2F;code&gt;，bitmap都为1，因此&lt;strong&gt;可能存在&lt;&#x2F;strong&gt;，需要访问磁盘，得知它不存在。这次磁盘访问就是false positive带来的坏影响，多余的磁盘访问。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;oliverdding&#x2F;imaw.io&#x2F;main&#x2F;bloom-filter-speed-up-answers.drawio.svg&quot; alt=&quot;布隆过滤器加速查询&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-xue-tui-dao&quot;&gt;数学推导&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;k取何值时，false positive发生概率最小，此时概率为？&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;由于hash函数“选择”&lt;code&gt;bitmap&lt;&#x2F;code&gt;的位置是随机的，因此对于长度为m的&lt;code&gt;bitmap&lt;&#x2F;code&gt;而言，插入一个值时某个位被置1、0的概率分别是$P(1) = \frac{1}{m}$、$P(0) = 1 - \frac{1}{m}$&lt;&#x2F;p&gt;
&lt;p&gt;当k个hash函数置位后，对于任意一个位，仍然是0的概率是$$P&#x27;(0)=P(0)^{k}=(1 - \frac{1}{m})^{k}$$&lt;&#x2F;p&gt;
&lt;p&gt;对于n个key插入后，对于任意一个位，仍然是0的概率是$$P&#x27;&#x27;(0)=P&#x27;(0)^{n}=(1 - \frac{1}{m})^{kn}$$&lt;&#x2F;p&gt;
&lt;p&gt;由于有自然对数e的计算公式$$\lim_{x \to \infty}{(1-\frac{1}{x})^{-x}}=e$$&lt;&#x2F;p&gt;
&lt;p&gt;我们可以&lt;strong&gt;近似&lt;&#x2F;strong&gt;计算$P&#x27;&#x27;(0)$得到$$P&#x27;&#x27;(0)=((1-\frac{1}{m})^{-m})^{-\frac{kn}{m}}\approx{}e^{-\frac{kn}{m}}$$&lt;&#x2F;p&gt;
&lt;p&gt;因此，对于任意一个位，其被置1的概率是$$P&#x27;&#x27;(1)=1-P&#x27;&#x27;(0)\approx{}1-e^{-\frac{kn}{m}}$$&lt;&#x2F;p&gt;
&lt;p&gt;当alse positive发生时，是指它的k个hash函数得到的位置都为1。由于k个hash函数相互独立，我们可以计算出alse positive的概率为$$\varepsilon=(1-(1-\frac{1}{m})^{kn})^{k}\approx{}(1-e^{-\frac{kn}{m}})^{k}$$&lt;&#x2F;p&gt;
&lt;p&gt;由于m、n是用户给定的值，唯一变量就是k，我们需要指定k的值去最小化alse positive概率。&lt;&#x2F;p&gt;
&lt;p&gt;令$p=e^{-\frac{kn}{m}}$以及自然对数公式，我们有$$\varepsilon=(1-p)^{k}=e^{kln(1-p)}$$&lt;&#x2F;p&gt;
&lt;p&gt;此时变更为最小的$g=kln(1-p)$&lt;&#x2F;p&gt;
&lt;p&gt;无中生有下得到$$g=kln{}(1-p)=-\frac{m}{n}ln(e^{-\frac{kn}{m}})ln(1-p)=-\frac{m}{n}ln(p)ln(1-p)$$&lt;&#x2F;p&gt;
&lt;p&gt;根据对称性原理得知当$p=\frac{1}{2}$时g取得最小，此时有$$k=\frac{m}{n}ln(2)$$&lt;&#x2F;p&gt;
&lt;p&gt;插入回$$\varepsilon=(1-p)^{k}$$得到alse positive的最小值为$$\varepsilon_{min}=(\frac{1}{2})^{k}\approx{}(0.6185)^{\frac{m}{n}}$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ying-yong&quot;&gt;应用&lt;&#x2F;h2&gt;
&lt;p&gt;那么如何在项目中使用bloom filter呢？&lt;&#x2F;p&gt;
&lt;h3 id=&quot;yi-zhi-n-he-varepsilon&quot;&gt;已知$n$和$\varepsilon$&lt;&#x2F;h3&gt;
&lt;p&gt;$n$由系统输入决定，$\varepsilon$则是开发者平衡内存使用和性能的选择，因此可以通过传入这两个变量自动计算剩下的值。&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;根据公式$m=\lceil{}-\frac{nln(\varepsilon)}{(ln(2))^2}\rceil$计算m的值&lt;&#x2F;li&gt;
&lt;li&gt;根据公式$k=\lceil{}-log_2(\varepsilon)\rceil$计算k的值&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;que-xian&quot;&gt;缺陷&lt;&#x2F;h2&gt;
&lt;p&gt;基础bloom filter有着自己的局限性，因此出现了许多变种，它们为了特殊场景作出不同的定制，增加一定开销换取更多功能：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;无法删除元素：计数布隆过滤器&lt;&#x2F;li&gt;
&lt;li&gt;无法扩展：&lt;a href=&quot;&#x2F;posts&#x2F;bloom-filter-2&#x2F;&quot;&gt;可扩展布隆过滤器&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;can-kao&quot;&gt;参考&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]: Space&#x2F;Time Trade-offs in Hash Coding with Allowable Errors&lt;&#x2F;li&gt;
&lt;li&gt;[2]: Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol &lt;&#x2F;li&gt;
&lt;li&gt;[3]: Network Applications of Bloom Filters: A Survey&lt;&#x2F;li&gt;
&lt;li&gt;[4]: Bloom Filters by Example. https:&#x2F;&#x2F;llimllib.github.io&#x2F;bloomfilter-tutorial&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>Apache Arrow: 大数据在数据湖后的下一个风向标</title>
            <pubDate>Thu, 18 Nov 2021 10:59:50 +0800</pubDate>
            <link>https://charmer.fun/posts/apache-arrow-the-next-windfall-after-the-data-lake-in-big-data/</link>
            <guid>https://charmer.fun/posts/apache-arrow-the-next-windfall-after-the-data-lake-in-big-data/</guid>
            <description>&lt;h2 id=&quot;jie-shao&quot;&gt;介绍&lt;&#x2F;h2&gt;
&lt;p&gt;根据&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;overview&#x2F;&quot;&gt;官方文档&lt;&#x2F;a&gt;介绍，Arrow是&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;A language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;一句话概括，Arrow用于系统间高效交互数据的组件。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;arrowde-he-xin-neng-li&quot;&gt;Arrow的核心能力&lt;&#x2F;h3&gt;
&lt;p&gt;Arrow本身不是一个存储、执行引擎，它只是一个交互数据的基础库。比如可以用于以下组件&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;SQL执行引擎 (e.g., Drill and Impala)&lt;&#x2F;li&gt;
&lt;li&gt;数据分析系统 (e.g., Pandas and Spark)&lt;&#x2F;li&gt;
&lt;li&gt;流和队列系统 (e.g., Kafka and Storm)&lt;&#x2F;li&gt;
&lt;li&gt;存储系统 (e.g., Parquet, Kudu, Cassandra and HBase)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;bei-jing&quot;&gt;背景&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;每个事物的产生发展都有其历史原因，如果抛开目的去“学习”，犹如竹篮子打水-一场空&lt;&#x2F;p&gt;
&lt;p align=&quot;right&quot;&gt;- 我说的 ;)&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;让我们回到2008年，故事从那开始...&lt;&#x2F;p&gt;
&lt;h3 id=&quot;qi-yin&quot;&gt;起因&lt;&#x2F;h3&gt;
&lt;p&gt;Wes McKinney在2008年开启了Pandas项目，这个python中分析、操作数据的瑞士军刀。紧接着在2014年，Wes加入Cloudera公司，并着手研究如何让python可以“插入”所有的大数据组件和数据库，但是每个系统都有自己操作数据的方式，于是：&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Oh my gosh, I&#x27;m going to have to write a dozen different data converters to marshal data, convert data between Pandas each of these data processing systems from Spark, to Impala, to different file formats and HDFS, so it was basically this overwhelming problem.&amp;quot;&lt;&#x2F;p&gt;
&lt;p align=&quot;right&quot;&gt;- Wes McKinney&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;除此之外，在大数据科学领域，dataframe的概念随处可见，每个框架都将datafrme作为高层定义，代表一个表、一系列API...但是其底层的实现天壤悬隔，Wes完全无法复用代码。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;无法共享数据&lt;&#x2F;strong&gt;、&lt;strong&gt;无法共享代码&lt;&#x2F;strong&gt;这两个大难题暂时困住了Wes。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fa-zhan&quot;&gt;发展&lt;&#x2F;h3&gt;
&lt;p&gt;Wes开始设计一种table middleware，作为不同组件交换数据的中间层，一种表接口的标准(standardized table interface)。&lt;&#x2F;p&gt;
&lt;p&gt;接着来到2015年，Wes团队遇到了Jacques和Apache Drill社区的小伙伴们，两伙人不谋而合，开始了合作。&lt;&#x2F;p&gt;
&lt;p&gt;由于业界没有统一规范的定义，他们合作的首个项目就是设计出了一个内存表视图的标准，并在不同语言都给出实现以证明可以在不同语言中共享数据，也就是说，你可以高效地将数据从Java到C++，或者Python。&lt;&#x2F;p&gt;
&lt;p&gt;自此，arrow项目创立。&lt;&#x2F;p&gt;
&lt;p&gt;在项目早期，最重要的是设计出一套与语言无关的内存表结构，并一定要方便分析处理。除此之外，还需要将各种格式、类型的数据转换、转出为这个标准格式的库。最后，还需要一个计算处理的库，以便于直接基于arrow进行快速数据分析处理。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;An important thing to remember about the project is that it&#x27;s front-end agnostic. So it&#x27;s not a new data frame library, like Pandas. It&#x27;s not a new database.&amp;quot;&lt;&#x2F;p&gt;
&lt;p align=&quot;right&quot;&gt;- Wes McKinney&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;此外，Wes在和Apache Impala团队合作的时候，发现Impala的代码中有大量和pandas做相似事情的片段，比如CSV序列化、反序列化的，I&#x2F;O子系统，自己的查询引擎，甚至自己的前端。在有了这样一个语言无关的内存数据格式，他们开始思考如何避免重复代码。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shi-xian&quot;&gt;实现&lt;&#x2F;h2&gt;
&lt;p&gt;故事讲完了，现在让我们一起来探索下arrow的设计。&lt;&#x2F;p&gt;
&lt;p&gt;面对不同语言、不同大数据组件之间的差异，首先我们肯定需要一个中间的表示来避免我们的后端直面差异，也就是前文提到的语言无关的内存表视图，这里就有一个必须挖掘的点，为了批量数据分析，我们应当选择&lt;strong&gt;列式存储&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;BNGPznF9xUy4qMi.png&quot; alt=&quot;列存表查询&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;使用列存的方式不仅减少了扫描内存的page数，还可以利用现在计算机SIMD(Single Instruction, Multiple Data)指令进行加速。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;扩展阅读 - &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;dbmsmusings.blogspot.com&#x2F;2017&#x2F;10&#x2F;apache-arrow-vs-parquet-and-orc-do-we.html&quot;&gt;Daniel Abadi的实验&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Daniel在亚马逊的EC2 t2.medium机器上创建了一个有60,000,000行数据的内存表。表由6个int32列组成，整个表大概由1.5GB。他创建了行表和列表两个实例，并对两种表进行简单地filter某个值。&lt;&#x2F;p&gt;
&lt;p&gt;在未开CPU优化的情况下，得到结果：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;wjkzO3cfR1BAdiE.png&quot; alt=&quot;无SIMD&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;行表和列表查询耗时相差无几。对于行表，每行都需要扫描，即使只使用到第一列；对于列表则只需要扫描第一列，按理说列表应该是行表的6倍快，但是在这个实验中由于CPU是瓶颈，而不是内存发往CPU的数据。&lt;&#x2F;p&gt;
&lt;p&gt;但是开启SIMD后，结果如下：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;ysv8F1SCMaQHEOw.png&quot; alt=&quot;开SIMD&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;SIMD可以同时比较多个数值（这里是4个数，差不多3倍快），减少打乱流水线的情况&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;现在我们可以继续考虑如何设计语言无关的内存表结构了&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;Ca6DOkIohglrXx4.png&quot; alt=&quot;直接IPC&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Arrow需要作为通用的传输结构&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;uJ5fAIYCXZK9eND.png&quot; alt=&quot;通过arrow交互&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;可是代码共享该如何实现呢？Arrow不应该是json、protobuf之流，后者适用于磁盘层面的数据存储交互。Arrow应当作为各个语言、组件中的一种数据格式库，应该是运行时的数据存储交互！直接可以操作数据，存取、计算：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;13&#x2F;pGxX76nHLvmNPYy.png&quot; alt=&quot;数据操作&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;arrowlie-ge-shi&quot;&gt;Arrow列格式&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;🚧 本节内容翻译整理自apache&#x2F;arrow代码仓库中&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;arrow&#x2F;blob&#x2F;master&#x2F;docs&#x2F;source&#x2F;format&#x2F;Columnar.rst&quot;&gt;Arrow Columnar Format规范&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Arrow列格式包含三部分：与语言无关的内存数据结构规范、元数据序列化以及一个用于序列化和通用数据传输的协议。&lt;&#x2F;p&gt;
&lt;p&gt;该列格式支持：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;顺序访问的数据&lt;&#x2F;li&gt;
&lt;li&gt;O(1)的随机读写&lt;&#x2F;li&gt;
&lt;li&gt;支持SIMD，向量化操作友好&lt;&#x2F;li&gt;
&lt;li&gt;可重新定位而无“pointer swizzling”问题，允许在共享内存中zero-copy&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;扩展阅读 - &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pointer_swizzling&quot;&gt;pointer swizzling&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;简单来说，内存中指针所指向的地址在写入磁盘（序列化）和从磁盘载入指针数据（反序列化）时，需要通过某种方式（swizzling和unswizzling）来使得指针存储的地址信息有效。&lt;&#x2F;p&gt;
&lt;p&gt;扩展阅读 - &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Zero-copy&quot;&gt;零拷贝&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;zero-copy（零拷贝）不是指真的没有拷贝了，而是说减少了不必要的数据拷贝与上下文切换（系统调用）。比如正常情况下用户态进程希望从磁盘中读取数据并写入socket，此时需要数据流经过磁盘-&amp;gt;系统态内存-&amp;gt;用户态内存-&amp;gt;系统态内存-&amp;gt;socket，发生了两次系统调用(磁盘的read()和写入socket的write())。使用系统提供的零拷贝函数(比如sendfile())则可以缩减为磁盘-&amp;gt;系统态内存-&amp;gt;socket。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;在Arrow中，最基本的结构是array(或者叫vector，是由一列相同类型的值组成，长度必须已知，且有上限；换个常见的叫法是field，字段)，每个array都有如下几个部分组成：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;逻辑上的数据类型（记录array类型）&lt;&#x2F;li&gt;
&lt;li&gt;一列缓冲区（存放具体数字、null）&lt;&#x2F;li&gt;
&lt;li&gt;一个长度为64位带符号的整数（记录array长度，也可以是32位）&lt;&#x2F;li&gt;
&lt;li&gt;另一个长度为64位的带符号的整数（记录null值的数量）&lt;&#x2F;li&gt;
&lt;li&gt;（可选）字典（用于字典编码的array）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Arrow还支持嵌套array类型，其实就是一列array组成，它们叫做子array(child arrays)。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wu-li-nei-cun-bu-ju&quot;&gt;物理内存布局&lt;&#x2F;h3&gt;
&lt;p&gt;每一个逻辑类型都有一个定义明确的物理布局，Arrow定义了如下物理布局：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Primitive(fixed-size)：用于存放具有相同长度的数值&lt;&#x2F;li&gt;
&lt;li&gt;Variable-size Binary：用于存放长度可变的数值。支持32位和64位的长度编码&lt;&#x2F;li&gt;
&lt;li&gt;Fixed-size List：嵌套类型，但是每个子array长度必须相同&lt;&#x2F;li&gt;
&lt;li&gt;Variable-size List：嵌套类型，每个子array长度可以不一致。支持32位和64位的长度编码&lt;&#x2F;li&gt;
&lt;li&gt;Struct：嵌套类型，由一组长度相同的命名子字段组成，但子字段的类型可以不一致。&lt;&#x2F;li&gt;
&lt;li&gt;Spare和Dense Union：嵌套类型，但是只有一组array，每个数值的类型是子类型集合之一&lt;&#x2F;li&gt;
&lt;li&gt;Null：存放一组null值，逻辑类型只能是null&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;bu-ju-li-zi&quot;&gt;布局例子&lt;&#x2F;h4&gt;
&lt;p&gt;本小节以Fixed-size Primitive Layout为例子讲述Arrow最基础的内存布局。&lt;&#x2F;p&gt;
&lt;p&gt;如前文所述，Primitive类型的数值槽长度相同，只能存放固定长度的数值，可以是字节或者比特。&lt;&#x2F;p&gt;
&lt;p&gt;放到具体内存布局上，本类型包含一个连续的内存缓冲区，总大小则是槽宽*长度（对于比特的槽宽，则需要四舍五入到字节）。&lt;&#x2F;p&gt;
&lt;p&gt;给出文档中一个Int32 Array的例子：&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#282828;color:#fdf4c1aa;&quot;&gt;&lt;code&gt;&lt;span&gt;[1, null, 2, 4, 8]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;会这样表示：&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#282828;color:#fdf4c1aa;&quot;&gt;&lt;code&gt;&lt;span&gt;* Length: 5, Null count: 1
&lt;&#x2F;span&gt;&lt;span&gt;* Validity bitmap buffer:
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  |Byte 0 (validity bitmap) | Bytes 1-63            |
&lt;&#x2F;span&gt;&lt;span&gt;  |-------------------------|-----------------------|
&lt;&#x2F;span&gt;&lt;span&gt;  | 00011101                | 0 (padding)           |
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;* Value Buffer:
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  |Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63 |
&lt;&#x2F;span&gt;&lt;span&gt;  |------------|-------------|-------------|-------------|-------------|-------------|
&lt;&#x2F;span&gt;&lt;span&gt;  | 1          | unspecified | 2           | 4           | 8           | unspecified |
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;其中有效性位图是用于记录每个值槽是否为空的。具体看&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;docs&#x2F;format&#x2F;Columnar.html#validity-bitmaps&quot;&gt;规范&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;剩下的布局都在Primitive布局上变化而来，具体看规范。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;bu-ju-shi-yong-de-huan-chong-qu&quot;&gt;布局使用的缓冲区&lt;&#x2F;h4&gt;
&lt;p&gt;Arrow的几种物理布局用到的缓冲区如下表所示：&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Layout Type&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Buffer 0&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Buffer 1&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Buffer 2&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Primitive&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;data&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Variable Binary&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;offsets&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;data&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;List&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;offsets&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Fixed-size List&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Struct&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Sparse Union&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;type ids&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Dense Union&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;type ids&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;offsets&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Null&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;Dictionary-encoded&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;validity&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;data (indices)&lt;&#x2F;td&gt;&lt;td align=&quot;center&quot;&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Arrow如何实现O(1)读写的呢？&lt;&#x2F;p&gt;
&lt;p&gt;所有的物理布局底层都是用数组存储数据，并且会根据层级嵌套建立offsets bitmap，当然就实现了O(1)的读写速度了。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;luo-ji-lei-xing&quot;&gt;逻辑类型&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;arrow&#x2F;blob&#x2F;master&#x2F;format&#x2F;Schema.fbs&quot;&gt;Schema.fbs&lt;&#x2F;a&gt;定义了Arrow支持的逻辑类型，每种逻辑类型都会对应到一种物理布局。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;xu-lie-hua-yu-ipc&quot;&gt;序列化与IPC&lt;&#x2F;h3&gt;
&lt;p&gt;列式格式序列化时最原始的单位是&amp;quot;record batch&amp;quot;(也就是一个表，table啦)。一个record batch是一组有序的array的集合，被称为record batch的字段(fields)。每个字段(field)有相同的长度，但是字段的数据类型可以不一样。record batch的字段名、类型构成了它的schema。&lt;&#x2F;p&gt;
&lt;p&gt;本节描述一个协议，用于将record batch序列化为二进制流，并可以无需内存拷贝重构record batch。&lt;&#x2F;p&gt;
&lt;p&gt;序列化时会分为这三部分：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Schema&lt;&#x2F;li&gt;
&lt;li&gt;RecordBatch&lt;&#x2F;li&gt;
&lt;li&gt;DictionaryBatch&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;这里我们只提及前两个。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;16&#x2F;ptCPLjx7bcWOXmi.png&quot; alt=&quot;布局&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;一个schema message和多个record batch message就能完整表示一个record batch。其中schema message存储表结构，record batch message存储字段metadata和字段值。&lt;&#x2F;p&gt;
&lt;p&gt;值得注意的是，record batch message包含实际的数据缓冲区、对应的物理内存布局。&lt;&#x2F;p&gt;
&lt;p&gt;然后问题又来了，Arrow为何无需pointer-swizzling即可实现流与数据转换的呢？答案就是message的metadata中存储了每个缓冲区的位置和大小，因此可以字节通过指针计算来重建Array数据结构，同时还避免了内存拷贝。&lt;&#x2F;p&gt;
&lt;p&gt;于是定义IPC流格式：&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#282828;color:#fdf4c1aa;&quot;&gt;&lt;code&gt;&lt;span&gt;&amp;lt;SCHEMA&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;DICTIONARY 0&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;DICTIONARY k - 1&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;RECORD BATCH 0&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;DICTIONARY x DELTA&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;DICTIONARY y DELTA&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;RECORD BATCH n - 1&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;EOS [optional]: 0xFFFFFFFF 0x00000000&amp;gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;由于这部分比较“定义”，本文不展开讲，更详细请看&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;docs&#x2F;format&#x2F;Columnar.html#ipc-streaming-format&quot;&gt;规范&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;arrow-flight&quot;&gt;Arrow Flight&lt;&#x2F;h2&gt;
&lt;p&gt;近段时间Arrow最大的变化就是添加了Flight，一个通用C&#x2F;S架构的高性能数据传输框架。Flight基于gRPC开发，从最开始重点就是优化Arrow格式数据。&lt;&#x2F;p&gt;
&lt;p&gt;Flight的具体细节请看&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;blog&#x2F;2019&#x2F;10&#x2F;13&#x2F;introducing-arrow-flight&#x2F;&quot;&gt;官方文档&lt;&#x2F;a&gt;。这里只介绍它的优势：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;无序列化&#x2F;反序列化：Flight会直接将内存中的Arrow发送，不进行任何序列化&#x2F;反序列化操作&lt;&#x2F;li&gt;
&lt;li&gt;批处理：Flight对record batch的操作无需访问具体的列、记录或者元素&lt;&#x2F;li&gt;
&lt;li&gt;高并发：Flight的吞吐量只收到客户端和服务端的吞吐量以及网络的限制&lt;&#x2F;li&gt;
&lt;li&gt;网络利用率高：Flight使用基于HTTP&#x2F;2的gRPC，不仅是快&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.dremio.com&#x2F;is-time-to-replace-odbc-jdbc&#x2F;&quot;&gt;官方给出的数据&lt;&#x2F;a&gt;是Flight的传输大约是标准ODBC的20-50倍。&lt;&#x2F;p&gt;
&lt;p&gt;对每个batch record平均行数256K时，在单节点传输时的性能对比（因为flight多节点时可以平行传输数据流）：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;16&#x2F;EOrNiXqY9Le2jTy.png&quot; alt=&quot;性能对比&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;shi-yong-chang-jing&quot;&gt;使用场景&lt;&#x2F;h3&gt;
&lt;p&gt;最过经典的非&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;blog&#x2F;2017&#x2F;07&#x2F;26&#x2F;spark-arrow&#x2F;&quot;&gt;PySpark&lt;&#x2F;a&gt;莫属，此外还有&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;blog&#x2F;2019&#x2F;01&#x2F;25&#x2F;r-spark-improvements&#x2F;&quot;&gt;sparklyr&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;另外，ClickHouse也有计划实现Arrow Flight的server端，一旦落地可用，spark与clickhouse交互就可以抛弃3G网般的JDBC了~&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;p&gt;本文从Arrow立项的背景入手，再到Arrow实现所需的设计，最后到Arrow具体columnar格式定义，介绍了Arrow的各种相关概念。最后补上一张图作为Arrow的优点、限制的总结：&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;11&#x2F;14&#x2F;i9hQ5wd4sj6y8Tf.png&quot; alt=&quot;总结&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;can-kao&quot;&gt;参考&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Wes和Jacques的视频访谈: &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.dremio.com&#x2F;starting-apache-arrow&#x2F;&quot;&gt;Starting Apache Arrow&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Arrow起名投票: &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;docs.google.com&#x2F;spreadsheets&#x2F;d&#x2F;1q6UqluW6SLuMKRwW2TBGBzHfYLlXYm37eKJlIxWQGQM&#x2F;edit#gid=0&quot;&gt;Vector Naming Discussion&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;思路来源: &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;tech.ipalfish.com&#x2F;blog&#x2F;2020&#x2F;12&#x2F;08&#x2F;apache_arrow_summary&#x2F;&quot;&gt;伴鱼技术团队&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;arrow&#x2F;blob&#x2F;master&#x2F;docs&#x2F;source&#x2F;format&#x2F;Columnar.rst&quot;&gt;Arrow Columnar Format&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;faq&#x2F;&quot;&gt;Arrow FAQ&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</description>
        </item>
        <item>
            <title>论文阅读笔记：RDD</title>
            <pubDate>Tue, 12 Oct 2021 17:31:07 +0800</pubDate>
            <link>https://charmer.fun/posts/paper-reading-rdd/</link>
            <guid>https://charmer.fun/posts/paper-reading-rdd/</guid>
            <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;&#x2F;h2&gt;
&lt;p&gt;RDD起源于现有的大数据框架对&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;迭代算法: 数据复用&lt;&#x2F;li&gt;
&lt;li&gt;交互式数据&lt;&#x2F;li&gt;
&lt;li&gt;挖掘: 数据共享&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;支持过差&lt;&#x2F;p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;新提出的Pregel、HaLoop虽然依次针对性提出解决方案，但是方案缺乏统一的抽象接口，无法推广。&lt;&#x2F;p&gt;
&lt;p&gt;于是，研究人员提出RDD，支持大量场景下内存复用。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Resilient Distributed Datasets(RDDs): RDDS are fault-tolerant, parallel data structures that enable users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manaipulate them using a rich set of operations.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;最大难题是设计接口，如何设计错误容忍度高的接口呢？参考现有设计，DSM、KV-stores、databases和Piccolo，他们提供细粒度内存接口，导致想要支持错误恢复，必须：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;跨设备备份数据&lt;&#x2F;li&gt;
&lt;li&gt;跨设备日志更新&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;这对于大数据应用而言太昂贵了，于是，RDD被设计为粗粒度内存模型接口，提供transformations接口对不同数据执行某些操作操作。此时，备份变得简单，只要记录“血统”(每个RDD的transformations流程)。如果某个RDD的分区丢失了，完全可以只恢复这个分区的RDD数据。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;rdd&quot;&gt;RDD&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;rddgai-shu&quot;&gt;RDD概述&lt;&#x2F;h3&gt;
&lt;p&gt;RDD是只读的数据分区集合。每个RDD只有三个来源：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;从内存对象集合生成&lt;&#x2F;li&gt;
&lt;li&gt;从外存数据生成&lt;&#x2F;li&gt;
&lt;li&gt;从现有RDD转换&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;用户拿到RDD不一定真实存在，可能只是这个RDD的“血统”。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;A program cannot reference an RDD that it cannot reconstruct after a failure.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;sparkbian-cheng-mo-xing&quot;&gt;Spark编程模型&lt;&#x2F;h3&gt;
&lt;p&gt;spark程序往往起始于通过transformations创建RDD，终止于通过actions处理RDD返回数值结果或者写入存储。&lt;&#x2F;p&gt;
&lt;p&gt;RDD不一定是实体的，在action被运行前才被计算，因此可以排排站，吃果果。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;rdddui-bi-dsmde-you-dian&quot;&gt;RDD对比DSM的优点&lt;&#x2F;h3&gt;
&lt;p&gt;Distributed Shared Memory(DSM): 另一种分布式框架，细粒度内存共享。&lt;&#x2F;p&gt;
&lt;p&gt;RDD的优点：&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;更高效的错误容忍度&lt;&#x2F;li&gt;
&lt;li&gt;得益于RDD不可变，可以运行时同时备份数据&lt;&#x2F;li&gt;
&lt;li&gt;任务可以被调度到离数据物理更近的节点上&lt;&#x2F;li&gt;
&lt;li&gt;当内存不够时RDD可以优雅缩减RDD数量规模&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;bu-gua-he-yu-rddde-chang-jing&quot;&gt;不适合于RDD的场景&lt;&#x2F;h3&gt;
&lt;p&gt;RDD只适合于&lt;strong&gt;批处理&lt;&#x2F;strong&gt;应用。&lt;&#x2F;p&gt;
&lt;p&gt;RDD不适合于异步细粒度更新共享状态的应用。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sparkbian-cheng-jie-kou&quot;&gt;Spark编程接口&lt;&#x2F;h2&gt;
&lt;p&gt;开发人员主要写&lt;code&gt;driver program&lt;&#x2F;code&gt;，用于连接集群中的workers。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;sparkzhong-rddde-cao-zuo&quot;&gt;Spark中RDD的操作&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;transformations&lt;&#x2F;code&gt;: 定义RDD的懒操作&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;actions&lt;&#x2F;code&gt;: 执行计算，返回结果或者写入存储&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;rddshi-xian&quot;&gt;RDD实现&lt;&#x2F;h2&gt;
&lt;p&gt;提出五个需要实现的接口。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;partitions()&lt;&#x2F;li&gt;
&lt;li&gt;preferredLocations(p)&lt;&#x2F;li&gt;
&lt;li&gt;dependencies()&lt;&#x2F;li&gt;
&lt;li&gt;iterator(p,parentIters)&lt;&#x2F;li&gt;
&lt;li&gt;partitioner()&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;依赖分位窄依赖和宽依赖。&lt;&#x2F;p&gt;
&lt;p&gt;为什么要分窄依赖？&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;可以管道执行&lt;&#x2F;li&gt;
&lt;li&gt;恢复更简单&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</description>
        </item>
        <item>
            <title>论文阅读笔记：HDFS</title>
            <pubDate>Fri, 17 Sep 2021 13:17:46 +0800</pubDate>
            <link>https://charmer.fun/posts/paper-reading-hdfs/</link>
            <guid>https://charmer.fun/posts/paper-reading-hdfs/</guid>
            <description>&lt;p&gt;&lt;mark&gt;注意: 本文涉及到的专有名词将直接使用英文, 便于理解记忆&lt;&#x2F;mark&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;mark&gt;注意: 本文面向hadoop 2版本, 其它版本差异并不涉及&lt;&#x2F;mark&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Hadoop是什么? SAS给出的描述[6]是:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hadoop is an open-source software framework for storing data and running applications on clusters of commodity hardware. It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;不难看出, 其中commodity hardware, massive storage和enormous processing power就是Hadoop的重要特点. 而The Hadoop Distributed File System(HDFS)作为Hadoop的核心子项目之一, 是Google File System(GFS)的实现, 为分布式计算提供数据存储和管理的功能.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bei-jing&quot;&gt;背景&lt;&#x2F;h2&gt;
&lt;p&gt;讲到HDFS就不得不提及GFS. 随着业务发展, Google公司对数据存储处理的需要日益增长, 便设计出GFS, 实现了在廉价的商品机型上高容错地存储数据, 并提供了高计算性能, 满足了Google公司的需要[5]. HDFS正是基于Google公司在2000年左右完成的GFS而改进实现的[1], 主要是改为通用文件系统(可以支持更多的存储系统而不只是Google的), 并为了内存一致性和性能, 修改了写入模型和写入流程, 其他几乎和GFS一致.&lt;&#x2F;p&gt;
&lt;p&gt;HDFS将metadata和data分开存放, 像其它的同类文件系统比如PVFS[4][7], Lustre[3]和GFS, HDFS将metadata存储于一个叫Name Node的专用服务器上, 将data存储于叫Data Node的服务器上. 所有的服务器都通过一个基于TCP的专用协议进行互联通信.&lt;&#x2F;p&gt;
&lt;p&gt;不同于Lustre和PVFS, HDFS的Data Node并不使用诸如RAID的数据保护策略, 而是仿照GFS, 将文件冗余存放于不同的Data Node. 这种方式除了提高数据的安全性, 还提高了系统的可用带宽, 因为随着文件冗余存储于不同的节点, 对于需要数据计算的软件而言, 数据正好在本地Data Node的概率更高.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mu-de&quot;&gt;目的&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;硬件错误时的高容错性&lt;&#x2F;p&gt;
&lt;p&gt;对于面向成千上万的廉价机型的集群而言, 机器的硬件错误是非常常见的. HDFS集群中每个节点都存储着文件系统的部分数据, 因此, 为了数据的安全性, HDFS需要能检测硬件错误, 并快速地, 自动地从错误中恢复.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;流式数据读写&lt;&#x2F;p&gt;
&lt;p&gt;许多应用需要流式数据访问, 但它们并不是运行在通用文件系统上的通用应用程序. 因此, HDFS面向批处理设计而不是普通用户交互访问, 也就是说, 重点是数据访问的高吞吐量而不是数据访问低延迟. 为了实现这一点, HDFS放宽了部分POSIX的要求.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;存储大数据集&lt;&#x2F;p&gt;
&lt;p&gt;存储在HDFS的文件可能是以GB, TB为单位的文件, HDFS provides high aggregate data bandwidth and can scale to hundreds of nodes in a single cluster.[2]&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;保证内存一致性&lt;&#x2F;p&gt;
&lt;p&gt;为了保证内存一致性, HDFS对读写访问模型做了简化, 区别于GFS, HDFS仅仅支持一个写入者和多个读入者. 并且一个文件一旦创建, 写入并关闭, 除了append和truncate外不能执行其他操作. 通过这些约束简化了内存一致性模型, 并支持高吞吐量数据访问.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;更强的可移植性&lt;&#x2F;p&gt;
&lt;p&gt;基于Java的HDFS, 不必多说.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;jia-gou&quot;&gt;架构&lt;&#x2F;h2&gt;
&lt;p&gt;这里放上一个视频, 比较生动形象, 先看一篇有个大概的印象再阅读文章.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; title=&quot;Understanding HDFS using Legos&quot; href=&quot;https:&#x2F;&#x2F;youtu.be&#x2F;4Gfl0WuONMY&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;res.cloudinary.com&#x2F;marcomontalbano&#x2F;image&#x2F;upload&#x2F;v1613722386&#x2F;video_to_markdown&#x2F;images&#x2F;youtube--4Gfl0WuONMY-c05b58ac6eb4c4700831b2b3070cd403.jpg&quot; alt=&quot;Understanding HDFS using Legos&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;下图是官网的HDFS架构图解.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r2.10.1&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;images&#x2F;hdfsarchitecture.png&quot; alt=&quot;HDFS Architecture&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;name-node&quot;&gt;Name Node&lt;&#x2F;h3&gt;
&lt;p&gt;HDFS的namespace是文件和目录的层次结构. 文件和目录在Name Node中以inode存储, 记录着像是权限, 修改访问时间, namespace和大小等等属性. 文件被切分为大的块(默认是128MB, 可以每个文件单独指定), 并且文件的每个块被独立冗余备份在不同的Data Node上(一般是3份, 同样也可以每个文件单独指定). Name Node维护namespace的树状结构, 并保存文件块到Data Node的映射. &lt;&#x2F;p&gt;
&lt;p&gt;当读取文件时, HDFS client先从Name Node获取存放文件数据块的Data Node, 然后直接从&lt;strong&gt;最近&lt;&#x2F;strong&gt;的Data Node获取数据. 同样的, 当写入数据时, 客户端会要求Name Node指定一组Data Node存储文件块和文件块的副本, 之后以管道的方式向Data Nodes写入数据.&lt;&#x2F;p&gt;
&lt;p&gt;在当前的设计中, 每个集群只有一个Name Node, 但有任意的Data Node和HDFS Client. HDFS将整个namespace存储于RAM中. inode数据和文件块的列表组成了系统的metadata, 被称为image. image被永久存储在本地文件系统, 被称为checkpoint. Name Node同样会存储image的修改日志, 被称为journal. 为了提高耐久性, checkpoint和journal可以冗余备份在不同的服务器. 当重启时, Name Node通过读入checkpoint并执行journal恢复原先的namespace. 块副本的位置可能会改变, 因此checkpoint不包括这部分.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;data-node&quot;&gt;Data Node&lt;&#x2F;h3&gt;
&lt;p&gt;每个块存储于Data Node时, 都由两个文件组成. 第一个包含了实际数据本身, 第二个是块的metadata, 包括校验码(checksum)和版本戳(generation stamp). 包含数据的文件的大小等同于块的真实大小, 不需要像传统文件系统那样需要额外空间以填充到标称块大小. 因此一个文件块若是只有一半的大小, 它就仅仅需要本地文件系统半块的的空间.&lt;&#x2F;p&gt;
&lt;p&gt;在startup阶段, 每个Data Node都会连接到Name Node并执行握手. 握手的目的之一是验证namespace ID和软件版本. 若任意一个没有匹配上, Data Node将会自动关机. 当HDFS文件系统格式化时会生成对应的namespace ID, 之后被存储于集群中所有节点上, 拥有不同的namespace ID的节点不可能在同一集群中, 保证了文件系统的完整性. 软件版本的一致性也是必要的, 因为不同版本的软件会导致数据损坏或丢失, 当更新时, 大集群中总会有节点未能正确关机, 更新, 导致软件版本不对, 这时候就需要关闭这些节点, 等后续手动操作.&lt;&#x2F;p&gt;
&lt;p&gt;刚刚初始化的Data Node, 可以加入任一集群并接受集群的namespace ID. 在握手后, Data Node注册到Name Node上. Data Node也存储着他自己独一无二的storage ID, 作为Data Node的内部标识, 保证了即使重启后, 被分配了不同的IP地址和端口, 仍能被正确识别. 而storage ID时Data Node注册到Name Node时被分配的, 以后&lt;strong&gt;不会再改变&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Data Node定期发送block report给Name Node, 包含block id, 版本戳和全部块的长度信息. 一经注册就会发送第一个block report. 之后每小时都会发送一次.&lt;&#x2F;p&gt;
&lt;p&gt;在运转过程中, Data Node会发送heartbeat给Name Node以告知操作正在进行并且数据正常. 默认的时间间隔是3秒. 若是10分钟内, Name Node未能收到Data Node的heartbeat信号, 将会认为Data Node离线并且上面的数据都丢失, 便会重新规划块备份.&lt;&#x2F;p&gt;
&lt;p&gt;事实上, heartbeat信号还包含了一些额外的信息, 包括中存储容量, 已使用存储的百分比, 正在处理的数据数量. 这些统计信息被Name Node用来平衡负载和安排空间.&lt;&#x2F;p&gt;
&lt;p&gt;Name Node并不会直接调度Data Node, 而是通过回复heartbeat的方式发送命令. 命令包括:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;往其他Data Node备份块&lt;&#x2F;li&gt;
&lt;li&gt;删除本地块备份&lt;&#x2F;li&gt;
&lt;li&gt;重新注册或关闭node&lt;&#x2F;li&gt;
&lt;li&gt;立刻发送block report&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;hdfs-client&quot;&gt;HDFS Client&lt;&#x2F;h3&gt;
&lt;p&gt;用户应用程序想要访问HDFS, 必须使用HDFS Client. 和传统文件系统类似, HDFS支持读, 写和删除文件, 以及创建和删除目录. 用户通过路径指定namespace内的文件和目录. 用户完全不用知道细节.&lt;&#x2F;p&gt;
&lt;p&gt;当应用读取文件时, HDFS Client会向Name Node请求存储着文件块的Data Node列表. 然后直接访问Data Node并获取数据. 当HDFS Client写入数据时, 首先请求Name Node选择第一个块的备份Data Node列表, Client会组织一个node-to-node的管道并写入数据. 当第一个块完成后, Client会再次请求Data Node列表, 再以同样的方式写后面的块.&lt;&#x2F;p&gt;
&lt;p&gt;不同于传统文件系统, HDFS提供获取文件块位置的API, 允许应用程序直接将任务运行于目标Data Node上, 减少了不必要的流量消耗. 同时, 允许应用程序设置文件的冗余因子, 默认情况下是3. 对于重要的文件和经常访问的文件, 可以设置为更高的值, 以保证数据安全和提高读带宽.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;imagehe-journal&quot;&gt;Image和Journal&lt;&#x2F;h3&gt;
&lt;p&gt;正如前文所说, image就是当前metadata的版本, 而checkpoint是写入到外存的image, 永远落后于当前RAM中的image, 而journal则记录着checkpoint到当前image之间的操作. checkpoint不会被修改, 只会在重启时被完整替换. 在startup阶段, Name Node从checkpoint中读取image, 并对其执行journal的操作, 以恢复上次的image. 当完成后且开始运作之前, 新的checkpoint和空白的journal文件会被回写.&lt;&#x2F;p&gt;
&lt;p&gt;若是checkpoint或journal文件丢失或者损坏, namespace的信息就丢失了. 为了保护这重要的信息, HDFS可以存储checkpoint和journal文件在不同的存储目录.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;最佳实践: 将这两个文件存储在不同的卷, 甚至远程NFS服务器.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Name Node是多线程服务器, 可以同时处理多个Client的请求. 于是, 将事务写入外存成为了整个系统的瓶颈, 因为一个线程在同步地写入时, 其他线程只能等待. 为了进一步优化, Name Node通过批处理的方式写入事务. 当一个线程请求flush-and-sync操作时, 所有在等待处理的提交会被同时写入. 剩下的线程只需要去检查自己的事务是否被保存, 而不是都去flush-and-sync.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;checkpoint-node&quot;&gt;Checkpoint Node&lt;&#x2F;h3&gt;
&lt;p&gt;Name Node在HDFS中, 除了为Client提供服务外, 还可以作为Checkpoint Node和Backup Node.&lt;&#x2F;p&gt;
&lt;p&gt;Checkpoint Node定时的合并现有的checkpoint和journal, 创建新的checkpoint和空的journal. Checkpoint Node通常是由运行在其他主机, 因为它必须要有和Name Node相同的内存需求. 它会从Name Node下载最新的checkpoint和journal, 合并, 然后回写.&lt;&#x2F;p&gt;
&lt;p&gt;这种方式减少了startup阶段的耗时, 因为减少了journal文件的大小.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;最佳实践: 每日更新一个checkpoint.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;backup-node&quot;&gt;Backup Node&lt;&#x2F;h3&gt;
&lt;p&gt;这是HDFS的新特性. 这部分不详细说, 类似双机热备份一样, 只是只读的备份.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;geng-xin-yu-wen-jian-xi-tong-kuai-zhao&quot;&gt;更新与文件系统快照&lt;&#x2F;h3&gt;
&lt;p&gt;在软件更新过程中, 最容易出现数据损坏. 为了降低这种风险, HDFS拥有快照功能. 快照允许管理员永久地保存当前文件系统的状态, 因此当更新异常导致数据丢失损坏时可以恢复到快照的状态.&lt;&#x2F;p&gt;
&lt;p&gt;在系统开启时, 可以选择开启快照功能(只能同时保存一个快照). 当要求快照时, Name Node首先读取checkpoint和journal文件, 并在内存中合并, 然后将新checkpoint和空的journal写入新的路径, 旧的checkpoint和journal仍然未改变.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shu-ju-cao-zuo&quot;&gt;数据操作&lt;&#x2F;h2&gt;
&lt;p&gt;数据操作主要是文件的读取写入以及副本的放置策略, 算是HDFS的核心部分.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wen-jian-xie-ru&quot;&gt;文件写入&lt;&#x2F;h3&gt;
&lt;p&gt;前文已经提到, 当一个应用创建文件写入数据并关闭后, 内容不再可以改变, 除了append和truncate.&lt;&#x2F;p&gt;
&lt;p&gt;HDFS实现single-writer-multiple-reader模式. 也就是说, 当一个HDFS Client打开一个文件并写入数据时, 会暂时独占这个文件的, 我们称之为lease, 其它的client都不再可以写入数据. 写入数据的client通过heartbeat信号更新自己对文件的lease. 当文件关闭, lease被回收. lease的期限受到软限制(soft limit)和硬限制(hard limit). 在软限制过期之前, 写入数据的client独占这个文件. 当软限制过期, 而client未能关闭文件或者更新自己的lease, 另一个client就可以抢占lease. 当硬限制过期(一小时), 而client未能更新lease, HDFS会当作client已经退出, 并自动关闭文件, 回收lease. 注意, lease是写入独占, 其它client仍然可以读取文件.&lt;&#x2F;p&gt;
&lt;p&gt;HDFS的文件是由block组成的. 当需要新的block时, Name Node将分配一个独一无二的block ID并决定一组Data Node去存放block以及它的副本. Data Node之间会组织管道, 它们的顺序会自动最优化, 降低带宽损耗. 数据被以数据包的方式序列化传输. 当第一个数据包被填满(一般是64KB), 数据包会被推送到管道. 下一个数据包可以在收到ACK(由管道最后一个Data Node回复)之前推出(就是TCP). 已发送未收到ACK的包的数量被client的滑动窗口限制(这里是TCP的滑动窗口).&lt;&#x2F;p&gt;
&lt;p&gt;数据写入HDFS文件后, 在文件被关闭前, HDFS并不保证文件立马可见. 若是应用程序需要边写边可读, 可以通过&lt;code&gt;hflush&lt;&#x2F;code&gt;操作: 当前数据包被立马发送到管道, 并同步等待管道中的ACK回复再继续写入.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.loli.net&#x2F;2021&#x2F;09&#x2F;17&#x2F;gHwyJF6TKZ8ReNs.png&quot; alt=&quot;HDFS example&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;上图是HDFS写入的例子.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wen-jian-du-qu&quot;&gt;文件读取&lt;&#x2F;h3&gt;
&lt;p&gt;在一个由成千上万node组成的集群中, 错误(通常是存储错误)经常发生. 存储于Data Node上的块可能由于内存错误, 硬盘错误或者网络错误导致损坏. 为了避免这些, HDFS生成并存储每个数据block的checksum. checksum被client在读取数据时校验. 当client创建一个HDFS文件时, 同时会计算每个块的checksum并和数据一同发送到管道. 当HDFS读取文件时, 数据block文件和checksum会被同时发送给client, client将会计算校验. 若出错, client会通知Name Node数据算坏, 然后从另一个Data Node获取该block.&lt;&#x2F;p&gt;
&lt;p&gt;当client打开读取一个文件, client将获取块的存储列表, 并选择最近的Data Node读取数据. 当读取失败, 就会尝试下一个. 可能会有疑问, 什么时候读取会失败呢? 有三个原因:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;目标Data Node挂了.&lt;&#x2F;li&gt;
&lt;li&gt;目标Data Node并没有目标block了&lt;&#x2F;li&gt;
&lt;li&gt;目标block校验不通过&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;fu-ben-guan-li&quot;&gt;副本管理&lt;&#x2F;h3&gt;
&lt;p&gt;Name Node还需要保证每一时刻每个块都有足够数量的副本. 每次收到block report, Name Node都会检测是否有块的副本数量大于少于预设值. 当副本数量大于预设值, Name Node会选择删除副本, 依据尽量不减少拥有副本的rack(这里的rack, 我理解为物理上放置在同一地点, 网络上属于同一网络的群组)数量. 当副本数了小于预设值, Name Node会将它放到复制优先队列(replication priority queue). 一个后台进程会不断扫描该优先队列的队头并决定哪个Data Node放置副本.&lt;&#x2F;p&gt;
&lt;p&gt;这部分细节非常多, 写下会导致篇幅过大, 请读者自行深入.&lt;&#x2F;p&gt;
&lt;p&gt;可以参看本文的基础论文: Konstantin Shvachko, Hairong Kuang, Sanjay Radia, Robert Chansler. &amp;quot;The Hadoop Distributed File System&amp;quot;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;zong-jie&quot;&gt;总结&lt;&#x2F;h2&gt;
&lt;p&gt;综上, HDFS是基于GFS的开源分布式文件系统, 具有高容错性, 可以部署在廉价不可靠的机器上. 适合批处理, 大量数据集存储处理与流式文件访问.&lt;&#x2F;p&gt;
&lt;p&gt;当然HDFS也有着自己的局限, 世界上没有十全十美的事情, 就好像为了简化内存一致性模型HDFS不得不使用one-writer的模式, 又好像为了提高吞吐量不得不放松POSIX标准.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;不支持低延迟访问&lt;&#x2F;li&gt;
&lt;li&gt;不适合小文件存储&lt;&#x2F;li&gt;
&lt;li&gt;不支持并发写入&lt;&#x2F;li&gt;
&lt;li&gt;不支持修改&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;can-kao&quot;&gt;参考&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;[1]:Google File System (GFS) and Hadoop Distributed File System (HDFS). http:&#x2F;&#x2F;stg-tud.github.io&#x2F;ctbd&#x2F;2017&#x2F;CTBD_06_gfs-hdfs.pdf . p5&lt;&#x2F;li&gt;
&lt;li&gt;[2]:HDFS Architecture. https:&#x2F;&#x2F;hadoop.apache.org&#x2F;docs&#x2F;r2.10.1&#x2F;hadoop-project-dist&#x2F;hadoop-hdfs&#x2F;HdfsDesign.html&lt;&#x2F;li&gt;
&lt;li&gt;[3]:Lustre File System. http:&#x2F;&#x2F;www.lustre.org&lt;&#x2F;li&gt;
&lt;li&gt;[4]:P. H. Carns, W. B. Ligon III, R. B. Ross, and R. Thakur. “PVFS: A parallel file system for Linux clusters,” in Proc. of 4th Annual Linux Showcase and Conference, 2000, pp. 317–327.&lt;&#x2F;li&gt;
&lt;li&gt;[5]:S. Ghemawat, H. Gobioff, S. Leung. “The Google file system,” In Proc. of ACM Symposium on Operating Systems Principles, Lake George, NY, Oct 2003, pp 29–43.&lt;&#x2F;li&gt;
&lt;li&gt;[6]:What is Hadoop? - SAS. https:&#x2F;&#x2F;www.sas.com&#x2F;zh_cn&#x2F;insights&#x2F;big-data&#x2F;hadoop.html&lt;&#x2F;li&gt;
&lt;li&gt;[7]:W. Tantisiriroj, S. Patil, G. Gibson. “Data-intensive file systems for Internet services: A rose by any other name ...” Technical Report CMUPDL-08-114, Parallel Data Laboratory, Carnegie Mellon University, Pittsburgh, PA, October 2008.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>翻译：RDD编程指导</title>
            <pubDate>Sun, 04 Jul 2021 12:42:17 +0800</pubDate>
            <link>https://charmer.fun/posts/translation-rdd-programming-guide/</link>
            <guid>https://charmer.fun/posts/translation-rdd-programming-guide/</guid>
            <description>&lt;h2 id=&quot;zong-lan&quot;&gt;总览&lt;&#x2F;h2&gt;
&lt;p&gt;每个spark应用包含一个驱动程序，两个职责:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;运行用户的main函数&lt;&#x2F;li&gt;
&lt;li&gt;在集群上执行一些并行操作&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;div class=&quot;mermaid is-flex is-justify-content-center is-align-items-center&quot;&gt;graph TD;
  subgraph spark application;
  core[driver program] -.职责1.-&amp;gt; act1(run user&amp;#x27;s main function);
  core[driver program] -.职责2.-&amp;gt; act2(execute various parallel operations);
  end&lt;&#x2F;div&gt;
&lt;p&gt;同样，spark提供了两层抽象:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;resilient distributed dataset(RDD): 一组元素的集合，分段地存储在集群不同节点上，并且支持并行操作。
&lt;ol&gt;
&lt;li&gt;可以是分布式文件系统中的一个文件&lt;&#x2F;li&gt;
&lt;li&gt;可以是driver program中的一个scala集合&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;shared variables: 可以在并行操作中使用。
&lt;ol&gt;
&lt;li&gt;broadcast类型变量: 用于在不同节点地内存中存储一个值&lt;&#x2F;li&gt;
&lt;li&gt;accumulators类型变量: 只支持&amp;quot;add&amp;quot;操作的变量&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;chu-shi-hua-spark&quot;&gt;初始化spark&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;初始化&lt;code&gt;SparkConf&lt;&#x2F;code&gt;，包含目标应用的信息&lt;&#x2F;li&gt;
&lt;li&gt;使用&lt;code&gt;SparkConf&lt;&#x2F;code&gt;初始化&lt;code&gt;SparkContext&lt;&#x2F;code&gt;以连接cluster&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;resilient-distributed-datasets-rdds&quot;&gt;Resilient Distributed Datasets (RDDs)&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;parallelized-collections&quot;&gt;Parallelized Collections&lt;&#x2F;h3&gt;
&lt;p&gt;调用&lt;code&gt;SparkContext&lt;&#x2F;code&gt;的&lt;code&gt;parallelize&lt;&#x2F;code&gt;方法，传入一个普通集合以生产一个并行集合，之后便可以并行操作。&lt;&#x2F;p&gt;
&lt;p&gt;此外，还可以控制并行集合的分区数量，spark会为并行集合的每一个分区运行一个任务。一般spark会自动设置合适的分区，当然也可以&lt;code&gt;parallelize&lt;&#x2F;code&gt;方法控制。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;external-datasets&quot;&gt;External Datasets&lt;&#x2F;h3&gt;
&lt;p&gt;spark可以从hadoop支持的任何数据源创建RDD，并且支持hadoop支持的所有&lt;code&gt;inputFormat&lt;&#x2F;code&gt;，包括text files、SequenceFiles。&lt;&#x2F;p&gt;
&lt;p&gt;spark读取文件(&lt;code&gt;textFile&lt;&#x2F;code&gt;方法)的一些提示:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;使用本地文件系统上的文件，，比如保证每个node相同路径上也有这个文件。&lt;&#x2F;li&gt;
&lt;li&gt;spark所有的文件读取的方法，支持目录、压缩文件以及通配符。&lt;&#x2F;li&gt;
&lt;li&gt;可选的第二个参数用于控制分块数量。默认情况下，spark会根据文件block大小创建分块(hdfs下默认是128M)。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;除了&lt;code&gt;textFile&lt;&#x2F;code&gt;外，spark还提供了其它格式的数据读取方法:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;SparkContext.wholeTextFiles&lt;&#x2F;code&gt;可以读取包含大量小文件的目录，并以文件名:文件内容对的方式返回。&lt;&#x2F;li&gt;
&lt;li&gt;对于&lt;code&gt;SequenceFile&lt;&#x2F;code&gt;，使用&lt;code&gt;SparkContext.sequenceFile[K, V]&lt;&#x2F;code&gt;方法读取，K、V是文件中key和value的类型，并且应该是hadoop的&lt;code&gt;Writable&lt;&#x2F;code&gt;接口的实现者。&lt;&#x2F;li&gt;
&lt;li&gt;对于hadoop的&lt;code&gt;inputFormat&lt;&#x2F;code&gt;，可以使用&lt;code&gt;SparkContext.hadoopRDD&lt;&#x2F;code&gt;方法。&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;saveAsObjectFile&lt;&#x2F;code&gt;和&lt;code&gt;objectFile&lt;&#x2F;code&gt;方法用于保存RDD。&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;rddde-cao-zuo&quot;&gt;RDD的操作&lt;&#x2F;h3&gt;
&lt;p&gt;RDD支持两种类型的操作:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;转换(transformations): 从现有数据集中创建新的数据集&lt;&#x2F;li&gt;
&lt;li&gt;计算(actions): 在某个数据集上运行并获取结果&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;举例，map操作就是转换，传入某个数据集的每个元素，返回代表着新数据集的RDD。reduce就是计算，聚合RDD中所有元素，返回最终结果到driver program。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;spark中所有的转换操作都是lazy的&lt;&#x2F;strong&gt;，也就是说不会在调用时立马计算，而是在计算操作被执行时才去计算。&lt;&#x2F;p&gt;
&lt;p&gt;默认情况下，每次运行计算，所有转换过的RDD都会被重复计算。但是可以配置spark缓存。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ji-chu&quot;&gt;基础&lt;&#x2F;h4&gt;
&lt;p&gt;举例说明(网站上例子):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;JavaRDD&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;String&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; lines &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; sc.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;textFile(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;quot;data.txt&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;JavaRDD&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Integer&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; lineLengths &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; lines.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;map(s &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;-&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; s.length())&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt; totalLength &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; lineLengths.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;reduce((a, b) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;-&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; b)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;第一行定义了从文件中获取的RDD。注意，并未实际读取，而只是一个指针。&lt;&#x2F;p&gt;
&lt;p&gt;第二行对这个文件进行map转换操作。注意，因为懒加载，并未实际处理。&lt;&#x2F;p&gt;
&lt;p&gt;最后一行执行reduce计算操作。此时spark将计算任务分解为一个个task，运行在集群不同的节点上。每个节点在其part上运行map、reduce操作，并返回结果给driver program。&lt;&#x2F;p&gt;
&lt;p&gt;通过在reduce前调用&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span&gt;lineLengths.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;persist(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;StorageLevel&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;.MEMORY_ONLY())&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;可以将&lt;code&gt;lineLengths&lt;&#x2F;code&gt;RDD缓存起来，之后重复调用reduce时不必重复map。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;han-shu-chuan-di&quot;&gt;函数传递&lt;&#x2F;h4&gt;
&lt;p&gt;spark的api严重依赖于&lt;strong&gt;传递函数&lt;&#x2F;strong&gt;的方式执行任务。具体请看官方网站。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;li-jie-bi-bao&quot;&gt;理解闭包&lt;&#x2F;h4&gt;
&lt;p&gt;spark最难理解的是在群集中执行代码时&lt;em&gt;变量的范围&lt;&#x2F;em&gt;、&lt;em&gt;生命周期&lt;&#x2F;em&gt;和&lt;em&gt;方法&lt;&#x2F;em&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;最容易犯的错就是在变量的作用域外调用RDD的operations。&lt;&#x2F;p&gt;
&lt;h5 id=&quot;li-zi&quot;&gt;例子&lt;&#x2F;h5&gt;
&lt;pre data-lang=&quot;java&quot; style=&quot;background-color:#282828;color:#fdf4c1aa;&quot; class=&quot;language-java &quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;int&lt;&#x2F;span&gt;&lt;span&gt; counter &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d3869b;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;JavaRDD&lt;&#x2F;span&gt;&lt;span&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#8ec07c;&quot;&gt;Integer&lt;&#x2F;span&gt;&lt;span&gt;&amp;gt; rdd &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt; sc.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;parallelize(data)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#928374;&quot;&gt;&#x2F;&#x2F; Wrong: Don&amp;#39;t do this!!
&lt;&#x2F;span&gt;&lt;span&gt;rdd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;foreach(x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fa5c4b;&quot;&gt;-&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; counter &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;+=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; x)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt;println(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b8bb26;&quot;&gt;&amp;quot;Counter value: &amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#fe8019;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#fdf4c1;&quot;&gt; counter)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;这段代码的结果取决于是否运行在同一个JVM中。也就是说，是本地运行 (--master = local[n]) 还是集群运行（spark-submit）。&lt;&#x2F;p&gt;
&lt;p&gt;运行jobs时，spark会将RDD的operations分解为tasks，交由一个个executor执行。而在执行前，spark会构建task的闭包。闭包是对RDD执行计算时必须要用到的变量(例子中就是counter)和方法(例子中就是foreach())，会被序列化并发送给各个executor。&lt;&#x2F;p&gt;
&lt;p&gt;在集群模式，闭包使用到的变量会被拷贝进闭包，然后发送给executor执行，executo后续会对闭包内的counter进行处理，而不是driver program中的counter。也就是说，修改并不成功，最终结果还是0。而local模式则相反，task运行在相同JVM，counter的引用就是原变量，因此可以得到正确结果。&lt;&#x2F;p&gt;
&lt;p&gt;在这个场景中，为了得到正确结果，spark定义了accumulator类型变量，提供了跨节点累加值的机制。&lt;&#x2F;p&gt;
&lt;h5 id=&quot;shu-chu-rddyuan-su&quot;&gt;输出RDD元素&lt;&#x2F;h5&gt;
&lt;p&gt;RDD的另一种常见错误用法是通过forEach(println)和map(println)输出元素。在单例模式下正常运作，但是在集群模式下，打印的值会输出到executor的标准输出，而不是driver program的，因此也就看不到元素内容。&lt;&#x2F;p&gt;
&lt;p&gt;为了正确获取元素内容，需要先调用&lt;code&gt;collect()&lt;&#x2F;code&gt;函数以将元素收集到driver program，然后再调用输出函数。但是这容易造成OOM。另一种方案是&lt;code&gt;take()&lt;&#x2F;code&gt;函数，只收集一定数量的元素。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;key-valuedui&quot;&gt;Key-Value对&lt;&#x2F;h4&gt;
&lt;p&gt;虽然大多数Spark操作都适用于包含任何类型对象的RDDs，但有几个特殊的操作只适用于键值对的RDDs。最常见的是分布式 &amp;quot;洗牌(shuffle)&amp;quot;操作，如按键分组或聚合元素。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;transformations&quot;&gt;Transformations&lt;&#x2F;h4&gt;
&lt;p&gt;请到网页查看: &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;rdd-programming-guide.html#transformations&quot;&gt;transformations&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;actions&quot;&gt;Actions&lt;&#x2F;h4&gt;
&lt;p&gt;请到网页查看: &lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;http:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;rdd-programming-guide.html#actions&quot;&gt;actions&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xi-pai-cao-zuo&quot;&gt;洗牌操作&lt;&#x2F;h4&gt;
&lt;p&gt;Spark中的某些操作会触发一个被称为洗牌的事件。洗牌是Spark重新分配数据的机制，使其在不同的分区中以不同的方式分组。这通常涉及跨执行器和机器的数据复制，使洗牌成为一个复杂而昂贵的操作。&lt;&#x2F;p&gt;
&lt;h5 id=&quot;bei-jing&quot;&gt;背景&lt;&#x2F;h5&gt;
&lt;p&gt;为了解释洗牌操作，用&lt;code&gt;reduceByKey&lt;&#x2F;code&gt;来演示，这个方法生成新的RDD，将所有于key关联的value合并为一个touple。最麻烦的地方是，不是所有和key关联的value在同一个分区、甚至节点。不同executor需要合作以得到结果。&lt;&#x2F;p&gt;
&lt;p&gt;在spark中，不会为了某个operation去交换分区。在计算过程中，一个task就只操作一个分区；因此，为了规划&lt;code&gt;reduceByKey&lt;&#x2F;code&gt;这个reduce任务，spark需要一个&lt;strong&gt;all-to-all&lt;&#x2F;strong&gt;操作: 从所有分区读取所有key对应的所有的的value，然后跨集群聚集所有value以计算每个key对应的结果集。这就是&amp;quot;洗牌&amp;quot;。&lt;&#x2F;p&gt;
&lt;p&gt;洗牌后排序看网站&lt;&#x2F;p&gt;
&lt;h5 id=&quot;xing-neng-sun-shi&quot;&gt;性能损失&lt;&#x2F;h5&gt;
&lt;p&gt;洗牌操作非常消耗资源，需要磁盘I&#x2F;O、序列化和网络I&#x2F;O。为了聚集洗牌需要的数据，spark会启动一系列task，&amp;quot;map任务&amp;quot;用于组织数据，&amp;quot;reduce任务&amp;quot;用于聚合数据。&lt;&#x2F;p&gt;
&lt;p&gt;在spark内部，洗牌操作执行时，每个&amp;quot;map任务&amp;quot;会将结果保存在内存中(直到溢出，溢出则分块写入外存)，然后根据分区顺序排序后写入单一的文件内; &amp;quot;reduce任务&amp;quot;会读取这个文件。&lt;&#x2F;p&gt;
&lt;p&gt;洗牌还会产生大量临时文件，直到RDD生命周期结束并且相关资源被垃圾回收。注意，垃圾回收会在任务停止之后很久很久才进行，也就是说，长期的的spark job会导致大量存储消耗。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;chi-jiu-cun-chu&quot;&gt;持久存储&lt;&#x2F;h3&gt;
&lt;p&gt;RDD可以被持久化存储，每个节点都会将它计算的分区存储在内存中，并在该数据集（或由其衍生的数据集）上的其他操作中重复使用。&lt;&#x2F;p&gt;
&lt;p&gt;使用方法&lt;code&gt;persist()&lt;&#x2F;code&gt;或者&lt;code&gt;cache()&lt;&#x2F;code&gt;使RDD持久化。&lt;&#x2F;p&gt;
&lt;p&gt;除此之外，持久化存储级别是可以选择的，默认是内存。具体请看&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;rdd-programming-guide.html#rdd-persistence&quot;&gt;网站&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;xuan-ze-chi-jiu-hua-cun-chu-ji-bie&quot;&gt;选择持久化存储级别&lt;&#x2F;h4&gt;
&lt;p&gt;持久化存储关系到内存和CPU的效率，官方给出选择方法，具体请看&lt;a rel=&quot;noopener nofollow noreferrer&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;rdd-programming-guide.html#which-storage-level-to-choose&quot;&gt;网站&lt;&#x2F;a&gt;。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;yi-chu-shu-ju&quot;&gt;移除数据&lt;&#x2F;h4&gt;
&lt;p&gt;Spark监控缓存的使用，并依照LRU算法丢弃旧缓存。可以通过&lt;code&gt;unpersist()&lt;&#x2F;code&gt;方法取消缓存，默认异步完成；可以通过传入&lt;code&gt;blocking=true&lt;&#x2F;code&gt;控制为同步方式。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gong-xiang-bian-liang&quot;&gt;共享变量&lt;&#x2F;h2&gt;
&lt;p&gt;Spark操作的运行类似与函数调用，值传递。为了实现在节点运行的操作改变driver program的数值，spark提供了两种变量类型：broadcast variable、accumulator。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;broadcast-variables&quot;&gt;Broadcast Variables&lt;&#x2F;h3&gt;
&lt;p&gt;广播变量可以让每个节点保存一个只读变量在本地，而不是每次运行task传入。用于高效地给每个节点一些较大数据集。&lt;&#x2F;p&gt;
&lt;h3 id=&quot;accumulators&quot;&gt;Accumulators&lt;&#x2F;h3&gt;
&lt;p&gt;累加器是只通过关联和换元操作 &amp;quot;只增&amp;quot;的变量，因此可以有效地支持并行，可以用来实现计数器（如MapReduce）或总和。Spark原生支持数字类型的累加器，程序员可以增加对新类型的支持。&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>Comment Policy</title>
            <pubDate>Fri, 01 Jan 2021 00:00:00 +0800</pubDate>
            <link>https://charmer.fun/comment-policy/</link>
            <guid>https://charmer.fun/comment-policy/</guid>
            <description>&lt;p&gt;Welcome to my blog! To ensure that you have a good time over here at Aurora, please make sure to stick to the following guidelines:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Be polite – don&#x27;t be rude to anyone and respect others. If something you would say would get you bottled on the head offline, please don&#x27;t say it here. Remember there is a person behind the screen and that your words affect people.&lt;&#x2F;li&gt;
&lt;li&gt;Don&#x27;t troll. We appreciate and welcome criticism towards writers here, but comments that add no value to the discussion or that are merely insulting would be deleted without exception.&lt;&#x2F;li&gt;
&lt;li&gt;Spam? Goodbye. We understand that you may make thousands of dollars working from home, however, we are not interested. Please refrain from posting or we&#x27;ll give you a hand and show both you and your post the door.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;To put it in a simple sentence, our guidelines boil down to &amp;quot;Don&#x27;t be a dick&amp;quot;. If you don&#x27;t follow any of these guidelines, not only will our moderators remove the comment but we may also ban you from participating in the discussions.&lt;&#x2F;p&gt;
&lt;p&gt;If you find any comments which violate our guidelines, please make sure to &lt;code&gt;flag&lt;&#x2F;code&gt; or &lt;code&gt;mark as spam&lt;&#x2F;code&gt; – our moderators highly appreciate your support!&lt;&#x2F;p&gt;
&lt;p&gt;Happy commenting!&lt;&#x2F;p&gt;
</description>
        </item>
    </channel>
</rss>
